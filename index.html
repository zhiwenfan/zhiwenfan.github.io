<!DOCTYPE html>
<!-- saved from url=(0017)https://xzhou.me/ -->
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><!--<base target="_blank">-->
    <base href="." target="_blank">
    <link rel="preconnect" href="https://fonts.gstatic.com/">
    <link rel="stylesheet" type="text/css" data-href="https://fonts.googleapis.com/css?family=Lato">
    <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin="">
    <meta name="viewport" content="width=device-width">
    <title>Zhiwen Fan's Homepage</title>
    <meta name="description" content="I am a Ph.D. student in Electrical Computer Engineering at The University of Texas at Austin">
    <meta name="next-head-count" content="4">
    <link rel="preload" href="./Homepage_files/fd4c9bf86266e80fae27.css" as="style">
    <link rel="stylesheet" href="./Homepage_files/fd4c9bf86266e80fae27.css" data-n-g="">
    <link rel="preload" href="./Homepage_files/a515be4c2f49419f3769.css" as="style">
    <link rel="stylesheet" href="./Homepage_files/a515be4c2f49419f3769.css" data-n-p=""><noscript
        data-n-css=""></noscript>
    <script defer="" nomodule="" src="./Homepage_files/polyfills-a54b4f32bdc1ef890ddd.js.下载"></script>
    <script src="./Homepage_files/webpack-715970c8028b8d8e1f64.js.下载" defer=""></script>
    <script src="./Homepage_files/framework-64eb7138163e04c228e4.js.下载" defer=""></script>
    <script src="./Homepage_files/main-c94e7f60255631414010.js.下载" defer=""></script>
    <script src="./Homepage_files/_app-3fa27215a3c41987e6d2.js.下载" defer=""></script>
    <script src="./Homepage_files/688-b942890a87f9a08b0e31.js.下载" defer=""></script>
    <script src="./Homepage_files/index-335648998a7bdac0c719.js.下载" defer=""></script>
    <script src="./Homepage_files/_buildManifest.js.下载" defer=""></script>
    <script src="./Homepage_files/_ssgManifest.js.下载" defer=""></script>
    <style data-href="https://fonts.googleapis.com/css?family=Lato">
        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 400;
            src: url(https://fonts.gstatic.com/s/lato/v24/S6uyw4BMUTPHjx4wWA.woff) format('woff')
        }

        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 400;
            src: url(https://fonts.gstatic.com/s/lato/v24/S6uyw4BMUTPHjxAwXiWtFCfQ7A.woff2) format('woff2');
            unicode-range: U+0100-02AF, U+0304, U+0308, U+0329, U+1E00-1E9F, U+1EF2-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF
        }

        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 400;
            src: url(https://fonts.gstatic.com/s/lato/v24/S6uyw4BMUTPHjx4wXiWtFCc.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD
        }
    </style>
</head>

<body>

    <div id="__next">
        <div class="styles_container__1_WuM">
            <section>
                <div class="personal_profile__BdQI4"><img class="personal_portrait__BAkO0"
                        src="./Homepage_files/zhiwenfan_square.JPG" alt="pottrait">
                    <div class="personal_profileInfo__1Y4pW">
                        <h2 class="personal_name__1_mHK">Zhiwen ("Aaron") Fan</h2>
                        <!-- <h3 class="personal_chineseName__18aOD">樊志文</h3> -->
                        <h3 class="personal_worksFor__2L0De">University of Texas at Austin</h3>
                        <div class="personal_links__p_eYL"><span><a
                                    href="mailto:zhiwenfan@utexas.edu">Email</a></span>
                                    <span><a href="https://scholar.google.com.hk/citations?user=tdoBO3UAAAAJ&hl=en-us">Google Scholar</a></span>
                                    <span><a href="./Homepage_files/Zhiwen_Fan_UTAustin_CV.pdf">CV</a></span>
                                    <span><a href="https://x.com/WayneINR">Twitter</a></span>
                                </div>
                    </div>
                </div>
            </section>
            <section>
                <h2>About Me</h2>
                <div>
                    <p>I am a Ph.D. candidate in Electrical Computer Engineering at The University of Texas at Austin advised by <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang">Prof. Atlas Wang</a> at <a href="https://vita-group.github.io/">VITA group</a>. <br>
                        I work closely with <a href="https://profiles.stanford.edu/marco-pavone">Prof. Marco Pavone</a> and <a href="https://yuewang.xyz/">Prof. Yue Wang</a> on 3D end-to-end models with robust generalization capabilities; with <a href="https://www.ee.ucla.edu/achuta-kadambi/">Prof. Achuta Kadambi</a> on recovering 3D/4D signals that capture the space-time structure of our world from casually captured data; and with <a href="https://ece.gatech.edu/directory/callie-hao">Prof. Callie Hao</a> on hardware-software co-design.
                        I was the awardees of <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2022-north-america">Qualcomm Innovation Fellowship 2022</a>.
                    </p>


                </div>
            </section>
            <section>
                <section>
                <h2>Recent News</h2>
                <div style="height: 200px; overflow-y: auto; padding-right: 10px;">
                    <ul>
                        <li>I will serve as the Area Chair for NeurIPS 2025.</li>
                        <li>Our ICLR'25 (<a href="https://4k4dgen.github.io/">4K4DGen</a>) is selected as <strong>spotlight</strong> presentation. </li>
                        <li>Our NeurIPS'24 (<a href="https://lightgaussian.github.io/">LightGaussian</a>) is selected as <strong>spotlight</strong> presentation. </li>
                        <li>Our <a href="https://arxiv.org/abs/2212.14849">Symbolic Visual RL</a> was accepted by IEEE Trans. PAMI. </li>
                        <li>Our IROS'24 (<a href="https://arxiv.org/abs/2404.00923">Multi-modal 3DGS SLAM</a>)
                            is selected as <strong>oral pitch finalist</strong> presentation. </li>
                        <li>Our CVPR'24 (<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Feature_3DGS_Supercharging_3D_Gaussian_Splatting_to_Enable_Distilled_Feature_CVPR_2024_paper.pdf">Feature-3DGS</a>) is selected as <strong>highlight</strong> presentation. </li>
                        <li>Our CVPR'23 (<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_NeuralLift-360_Lifting_an_In-the-Wild_2D_Photo_to_a_3D_Object_CVPR_2023_paper.pdf">NeuralLift-360</a>) is selected as <strong>highlight</strong> presentation. </li>
                        <li>I was one of the awardees of the <strong><span style="color: darkblue;">Qualcomm Innovation Fellowship</span></strong> (North America) 2022 <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2022-north-america">(QIF 2022)</a>. Innovation title: "Real-time Visual Processing for Autonomous Driving via Video Transformer with Data-Model-Accelerator Tri-design". </li>
                        <li>We won 3rd place in the <strong><span style="color: darkblue;">University Demo Best Demonstration</span></strong> at the 59th Design Automation Conference <a href="https://www.dac.com/">(DAC 2022)</a>. We demo for a multi-task vision transformer on FPGA. </li>
                        <li>Our CVPR'22 (<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fan_CADTransformer_Panoptic_Symbol_Spotting_Transformer_for_CAD_Drawings_CVPR_2022_paper.pdf">CADTransformer</a>) is selected as <strong>oral</strong> presentation. </li>
                        <li>Our paper for CVPR'20 (<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Gu_Cascade_Cost_Volume_for_High-Resolution_Multi-View_Stereo_and_Stereo_Matching_CVPR_2020_paper.pdf">Cascade Cost Volume</a>) is selected as <strong>oral</strong> presentation. </li>
                    </ul>
                </div>
            </section>

            <style>
                /* Custom scrollbar styling */
                div::-webkit-scrollbar {
                    width: 8px;
                }

                div::-webkit-scrollbar-track {
                    background: #f1f1f1;
                    border-radius: 4px;
                }

                div::-webkit-scrollbar-thumb {
                    background: #888;
                    border-radius: 4px;
                }

                div::-webkit-scrollbar-thumb:hover {
                    background: #555;
                }
            </style>


            <section>
                <h2>Researches Interests<br class="publication-list_mobileBreak__24vsO"></h2>


                <div class="publication_publication__1Icb_">

                    <div class="publication_image_0dot9_size"><img src="./Homepage_files/research_overview.svg"
                        alt="loading...">
                </div>
            </div>
            My research goal is to enhance AI agents' ability to interact with the physical world through 3D AI.
            I focus on developing multi-modal 3D models that enable perception, generation, and action in 3D space.
            By integrating natural language, images, videos, and 3D data, my work aims to bridge AI agents,
            human instructions, and the physical world. I am particularly interested in building real-time 3D models
            that can understand, recreate, and interact with their environment using spatial awareness and common sense.


                <div>
                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__full_size"><video src="./Homepage_files/videos/homepage_applications.mp4"
                            title="Video demos for application is loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video>
                    </div>
                </div>
                <p>My research has been demonstrated on platforms such as Quest 3, implemented within IARPA projects, and integrated into multiple commercial products.</p>


            </section>

            <section>
                <h2>Selected Publications<br class="publication-list_mobileBreak__24vsO"><span
                        class="publication-list_filters__3ikvu"><span>Full publication list at </span><span><a
                                href="https://scholar.google.com.hk/citations?hl=en&user=tdoBO3UAAAAJ&view_op=list_works&sortby=pubdate">Google
                                Scholar</a></span></span></h2>
                <div class="publication-list_smallText__pUJXB">* denotes equal contribution, &dagger; denotes project lead.</div>

                <!-- Filter Buttons -->
                <div id="pub-filters">
                    <button class="filter-btn active" onclick="filterPublications('all')">All</button>
                    <button class="filter-btn" onclick="filterPublications('category-3d-foundation')">3D Modeling</button>
                    <button class="filter-btn" onclick="filterPublications('category-3d-vlm')">3D VLMs</button>
                    <button class="filter-btn" onclick="filterPublications('category-3d-aigc')">3D AIGC</button>
                    <button class="filter-btn" onclick="filterPublications('category-3d-human')">3D Human</button>
                    <button class="filter-btn" onclick="filterPublications('category-inverse')">Inverse Problems</button>
                </div>

                <div id="publications-list">

                    <!-- Category: 3D Foundation Models -->
                    <h3 class="publication-category-header category-3d-foundation">3D Modeling</h3>
                  
                    <!-- LightGaussian -->
                    <div class="publication-item category-3d-foundation">
                      <div class="publication_publication__1Icb_">
                        <video
                          class="publication_image__1EUuC"
                          src="./Homepage_files/videos/garden_lighgaussian.mp4"
                          title="LightGaussian video loading…"
                          playsinline autoplay loop preload muted>
                        </video>
                        <div class="publication_info__kLRGP">
                          <div class="publication_title__3m6SE">
                            LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS
                          </div>
                          <div class="publication_authors__qkFXc">
                            Zhiwen Fan*†, Kevin Wang*, Kairun Wen, Zehao Zhu, Dejia Xu, Zhangyang Wang
                          </div>
                          <div class="publication_venue__1Dv6R">
                            NeurIPS 2024 <span class="publication_highlights__2ILmf">(Spotlight, top 2.1% of 15 671)</span>
                          </div>
                          <div class="publication_links__aEpO_">
                            <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/fd881d3b625437354d4421818f81058f-Abstract-Conference.html">Paper</a> 
                            <a href="https://lightgaussian.github.io/">Project</a> 
                            <a href="https://github.com/VITA-Group/LightGaussian">Code</a>
                          </div>
                        </div>
                      </div>
                    </div>
                  
                    <!-- InstantSplat -->
                    <div class="publication-item category-3d-foundation">
                      <div class="publication_publication__1Icb_">
                        <video
                          class="publication_image__1EUuC"
                          src="./Homepage_files/videos/instantsplat.mp4"
                          title="InstantSplat video loading…"
                          playsinline autoplay loop preload muted>
                        </video>
                        <div class="publication_info__kLRGP">
                          <div class="publication_title__3m6SE">
                            InstantSplat: Sparse-view Pose‑free Gaussian Splatting in Seconds
                          </div>
                          <div class="publication_authors__qkFXc">
                            Zhiwen Fan*†, Wenyan Cong*, Kairun Wen*, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, Zhangyang Wang, Yue Wang
                          </div>
                          <div class="publication_venue__1Dv6R">
                            Preprint
                          </div>
                          <div class="publication_links__aEpO_">
                            <a href="https://arxiv.org/abs/2403.20309">Paper</a> 
                            <a href="https://instantsplat.github.io/">Project</a> 
                            <a href="https://github.com/NVlabs/InstantSplat">Code</a>
                          </div>
                        </div>
                      </div>
                    </div>
                  
                    <!-- MM3DGS SLAM -->
                    <div class="publication-item category-3d-foundation">
                      <div class="publication_publication__1Icb_">
                        <video
                          class="publication_image__1EUuC"
                          src="./Homepage_files/videos/mmgsslam.mp4"
                          title="MM3DGS SLAM video loading…"
                          playsinline autoplay loop preload muted>
                        </video>
                        <div class="publication_info__kLRGP">
                          <div class="publication_title__3m6SE">
                            MM3DGS SLAM: Multi‑modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements
                          </div>
                          <div class="publication_authors__qkFXc">
                            Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu
                          </div>
                          <div class="publication_venue__1Dv6R">
                            IROS 2024 <span class="publication_highlights__2ILmf">(Oral Pitch Highlight)</span>
                          </div>
                          <div class="publication_links__aEpO_">
                            <a href="https://arxiv.org/abs/2404.00923">Paper</a> 
                            <a href="https://vita-group.github.io/MM3DGS-SLAM/">Project</a> 
                            <a href="https://github.com/VITA-Group/MM3DGS-SLAM">Code</a>
                          </div>
                        </div>
                      </div>
                    </div>
                  
                    <!-- M^3ViT -->
                    <div class="publication-item category-3d-foundation">
                      <div class="publication_publication__1Icb_">
                        <img
                          class="publication_image__1EUuC"
                          src="./Homepage_files/videos/m3vit.png"
                          alt="M^3ViT thumbnail" />
                        <div class="publication_info__kLRGP">
                          <div class="publication_title__3m6SE">
                            M^3ViT: Mixture‑of‑Experts Vision Transformer for Efficient Multi‑task Learning with Model‑Accelerator Co‑design
                          </div>
                          <div class="publication_authors__qkFXc">
                            Zhiwen Fan*†, Hanxue Liang*, Rishov Sarkar, Ziyu Jiang, Tianlong Chen, Kai Zou, Yu Cheng, Cong Hao, Zhangyang Wang
                          </div>
                          <div class="publication_venue__1Dv6R">
                            NeurIPS 2022 <span class="publication_highlights__2ILmf">(QIF 2022 Award & DAC 3rd best demo)</span>
                          </div>
                          <div class="publication_links__aEpO_">
                            <a href="https://arxiv.org/pdf/2210.14793">Paper</a> 
                            <a href="https://github.com/VITA-Group/M3ViT">Code</a>
                          </div>
                        </div>
                      </div>
                    </div>
                  
                    <!-- Cascade Cost Volume -->
                    <div class="publication-item category-3d-foundation">
                      <div class="publication_publication__1Icb_">
                        <img
                          class="publication_image__1EUuC"
                          src="./Homepage_files/videos/cascadecostvolume.jpeg"
                          alt="Cascade Cost Volume thumbnail" />
                        <div class="publication_info__kLRGP">
                          <div class="publication_title__3m6SE">
                            Cascade Cost Volume for High‑Resolution Multi‑View Stereo and Stereo Matching
                          </div>
                          <div class="publication_authors__qkFXc">
                            Zhiwen Fan*, Xiaodong Gu*, Siyu Zhu, Zuozhuo Dai, Feitong Tan, Ping Tan
                          </div>
                          <div class="publication_venue__1Dv6R">
                            CVPR 2020 <span class="publication_highlights__2ILmf">(Oral Presentation, top 5% of submissions)</span>
                          </div>
                          <div class="publication_links__aEpO_">
                            <a href="https://arxiv.org/abs/1912.06378">Paper</a> 
                            <a href="https://www.youtube.com/watch?v=rcJiRQqDKbo">Video</a> 
                            <a href="https://github.com/alibaba/cascade-stereo">Code</a>
                          </div>
                        </div>
                      </div>
                    </div>
                  
                    <!-- CADTransformer -->
                    <div class="publication-item category-3d-foundation">
                      <div class="publication_publication__1Icb_">
                        <img
                          class="publication_image__1EUuC"
                          src="./Homepage_files/videos/floorplancad.png"
                          alt="CADTransformer thumbnail" />
                        <div class="publication_info__kLRGP">
                          <div class="publication_title__3m6SE">
                            CADTransformer: Panoptic Symbol Spotting Transformer for CAD Drawings
                          </div>
                          <div class="publication_authors__qkFXc">
                            Zhiwen Fan, Tianlong Chen, Peihao Wang, Zhangyang Wang
                          </div>
                          <div class="publication_venue__1Dv6R">
                            CVPR 2022 <span class="publication_highlights__2ILmf">(Oral Presentation, top 5% of submissions)</span>
                          </div>
                          <div class="publication_links__aEpO_">
                            <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Fan_CADTransformer_Panoptic_Symbol_Spotting_Transformer_for_CAD_Drawings_CVPR_2022_paper.html">Paper</a> 
                            <a href="https://github.com/VITA-Group/CADTransformer">Code</a>
                          </div>
                        </div>
                      </div>
                    </div>
                  
                  
                    <!-- Category: 3D VLMs -->
                    <h3 class="publication-category-header category-3d-vlm">3D VLMs</h3>
                  
                    <!-- VLM‑3R -->
                    <div class="publication-item category-3d-vlm">
                      <div class="publication_publication__1Icb_">
                        <img
                          class="publication_image__1EUuC"
                          src="./Homepage_files/VLM-3R.svg"
                          alt="VLM-3R thumbnail" />
                        <div class="publication_info__kLRGP">
                          <div class="publication_title__3m6SE">
                            VLM‑3R: Vision‑Language Models Augmented with Instruction‑Tuned 3D Reconstruction
                          </div>
                          <div class="publication_authors__qkFXc">
                            Zhiwen Fan <em>et al.</em>
                          </div>
                          <div class="publication_venue__1Dv6R">
                            Submitted
                          </div>
                        </div>
                      </div>
                    </div>
                  
                    <!-- Large Spatial Model -->
                    <div class="publication-item category-3d-vlm">
                      <div class="publication_publication__1Icb_">
                        <video
                          class="publication_image__1EUuC"
                          src="./Homepage_files/videos/lsm.mp4"
                          title="Large Spatial Model video loading…"
                          playsinline autoplay loop preload muted>
                        </video>
                        <div class="publication_info__kLRGP">
                          <div class="publication_title__3m6SE">
                            Large Spatial Model: Real‑time Unposed Images to Semantic 3D
                          </div>
                          <div class="publication_authors__qkFXc">
                            Zhiwen Fan*†, Jian Zhang*, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi, Zhangyang Wang, Danfei Xu, Boris Ivanovic, Marco Pavone, Yue Wang
                          </div>
                          <div class="publication_venue__1Dv6R">
                            NeurIPS 2024
                          </div>
                          <div class="publication_links__aEpO_">
                            <a href="https://openreview.net/forum?id=ybHPzL7eYT&referrer=%5Bthe%20profile%20of%20Yue%20Wang%5D(%2Fprofile%3Fid%3D~Yue_Wang2)">Paper</a> 
                            <a href="https://largespatialmodel.github.io/">Project</a> 
                            <a href="https://github.com/NVlabs/LSM">Code</a>
                          </div>
                        </div>
                      </div>
                    </div>
                  
                    <!-- Feature 3DGS -->
                    <div class="publication-item category-3d-vlm">
                      <div class="publication_publication__1Icb_">
                        <video
                          class="publication_image__1EUuC"
                          src="./Homepage_files/videos/feature-3dgs.mp4"
                          title="Feature 3DGS video loading…"
                          playsinline autoplay loop preload muted>
                        </video>
                        <div class="publication_info__kLRGP">
                          <div class="publication_title__3m6SE">
                            Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields
                          </div>
                          <div class="publication_authors__qkFXc">
                            Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, Achuta Kadambi
                          </div>
                          <div class="publication_venue__1Dv6R">
                            CVPR 2024 <span class="publication_highlights__2ILmf">(Highlight, 2.8% of 11 532)</span>
                          </div>
                          <div class="publication_links__aEpO_">
                            <a href="https://arxiv.org/abs/2312.03203">Paper</a> 
                            <a href="https://feature-3dgs.github.io/">Project</a> 
                            <a href="https://github.com/ShijieZhou-UCLA/feature-3dgs">Code</a>
                          </div>
                        </div>
                      </div>
                    </div>
                  
                    <!-- NeRF‑SOS -->
                    <div class="publication-item category-3d-vlm">
                      <div class="publication_publication__1Icb_">
                        <video
                          class="publication_image__1EUuC"
                          src="./Homepage_files/videos/nerf_sos.mp4"
                          title="NeRF‑SOS video loading…"
                          playsinline autoplay loop preload muted>
                        </video>
                        <div class="publication_info__kLRGP">
                          <div class="publication_title__3m6SE">
                            NeRF‑SOS: Any‑View Self‑supervised Object Segmentation from Complex Real‑World Scenes
                          </div>
                          <div class="publication_authors__qkFXc">
                            Zhiwen Fan, Peihao Wang, Yifan Jiang, Xinyu Gong, Dejia Xu, Zhangyang Wang
                          </div>
                          <div class="publication_venue__1Dv6R">
                            ICLR 2023
                          </div>
                          <div class="publication_links__aEpO_">
                            <a href="https://openreview.net/pdf?id=kfOtMqYJlUU">Paper</a> 
                            <a href="https://zhiwenfan.github.io/NeRF-SOS/">Project</a> 
                            <a href="https://github.com/VITA-Group/NeRF-SOS">Code</a>
                          </div>
                        </div>
                      </div>
                    </div>
                  
                  
                    <!-- Category: 3D AIGC -->
                    <h3 class="publication-category-header category-3d-aigc">3D AIGC</h3>
                  
                    <!-- Can Test‑Time Scaling Improve World Foundation Model? -->
                    <div class="publication-item category-3d-aigc">
                      <div class="publication_publication__1Icb_">
                        <video
                          class="publication_image__1EUuC"
                          src="./Homepage_files/videos/swift.mp4"
                          title="4K4DGen video loading…"
                          playsinline autoplay loop preload muted>
                        </video>
                        <div class="publication_info__kLRGP">
                          <div class="publication_title__3m6SE">
                            Can Test‑Time Scaling Improve World Foundation Model?
                          </div>
                          <div class="publication_authors__qkFXc">
                            Wenyan Cong, Hanqing Zhu, Peihao Wang, Bangya Liu, Dejia Xu, Kevin Wang, David Z. Pan, Yan Wang, Zhiwen Fan, Zhangyang Wang
                          </div>
                          <div class="publication_venue__1Dv6R">
                            Preprint
                          </div>
                          <div class="publication_links__aEpO_">
                            <a href="https://arxiv.org/abs/2503.24320">Paper</a> 
                            <a href="https://scalingwfm.github.io/">Project</a> 
                            <a href="https://github.com/Mia-Cong/SWIFT">Code</a>
                          </div>
                        </div>
                      </div>
                    </div>
                  
                    <!-- 4K4DGen -->
                    <div class="publication-item category-3d-aigc">
                      <div class="publication_publication__1Icb_">
                        <video
                          class="publication_image__1EUuC"
                          src="./Homepage_files/videos/4k4dgen.mp4"
                          title="4K4DGen video loading…"
                          playsinline autoplay loop preload muted>
                        </video>
                        <div class="publication_info__kLRGP">
                          <div class="publication_title__3m6SE">
                            4K4DGen: Panoramic 4D Generation at 4K Resolution
                          </div>
                          <div class="publication_authors__qkFXc">
                            Renjie Li, Panwang Pan, Dejia Xu, Shijie Zhou, Xuanyang Zhang, Zeming Li, Achuta Kadambi, Zhangyang Wang, Zhiwen Fan
                          </div>
                          <div class="publication_venue__1Dv6R">
                            ICLR 2025 <span class="publication_highlights__2ILmf">(Spotlight, 3.2% of 11 672)</span>
                          </div>
                          <div class="publication_links__aEpO_">
                            <a href="https://openreview.net/forum?id=qxRoo7ULCo">Paper</a> 
                            <a href="https://4k4dgen.github.io/">Project</a> 
                            <a href="https://4k4dgen.github.io/">Code</a>
                          </div>
                        </div>
                      </div>
                    </div>
                  
                    <!-- MoonSim -->
                    <div class="publication-item category-3d-aigc">
                      <div class="publication_publication__1Icb_">
                        <video
                          class="publication_image__1EUuC"
                          src="./Homepage_files/videos/moonsim.mp4"
                          title="MoonSim video loading…"
                          playsinline autoplay loop preload muted>
                        </video>
                        <div class="publication_info__kLRGP">
                          <div class="publication_title__3m6SE">
                            MoonSim: A Photorealistic Lunar Environment Simulator
                          </div>
                          <div class="publication_authors__qkFXc">
                            Ziyu Chen*†, Henghui Bao*, Ting‑Hsuan Chen*, Haozhe Lou, Ge Yang, Zhiwen Fan, Marco Pavone, Yue Wang
                          </div>
                          <div class="publication_venue__1Dv6R">
                            Under submission
                          </div>
                          <div class="publication_links__aEpO_">
                            <a href="https://anonymousi079j.github.io/moonsim/">Paper</a> 
                            <a href="https://anonymousi079j.github.io/moonsim/">Project</a> 
                            <a href="https://anonymousi079j.github.io/moonsim/">Code</a>
                          </div>
                        </div>
                      </div>
                    </div>
                  
                    <!-- DreamScene360 -->
                    <div class="publication-item category-3d-aigc">
                      <div class="publication_publication__1Icb_">
                        <video
                          class="publication_image__1EUuC"
                          src="./Homepage_files/videos/dreamscene360.mp4"
                          title="DreamScene360 video loading…"
                          playsinline autoplay loop preload muted>
                        </video>
                        <div class="publication_info__kLRGP">
                          <div class="publication_title__3m6SE">
                            DreamScene360: Unconstrained Text‑to‑3D Scene Generation with Panoramic Gaussian Splatting
                          </div>
                          <div class="publication_authors__qkFXc">
                            Shijie Zhou*†, Zhiwen Fan*†, Dejia Xu*, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi
                          </div>
                          <div class="publication_venue__1Dv6R">
                            ECCV 2024
                          </div>
                          <div class="publication_links__aEpO_">
                            <a href="https://arxiv.org/abs/2404.06903">Paper</a> 
                            <a href="https://dreamscene360.github.io/">Project</a> 
                            <a href="https://dreamscene360.github.io/">Code</a>
                          </div>
                        </div>
                      </div>
                    </div>
                  
                    <!-- Unified Implicit Neural Stylization -->
                    <div class="publication-item category-3d-aigc">
                      <div class="publication_publication__1Icb_">
                        <video
                          class="publication_image__1EUuC"
                          src="./Homepage_files/videos/INS_merge.mp4"
                          title="INS video loading…"
                          playsinline autoplay loop preload muted>
                        </video>
                        <div class="publication_info__kLRGP">
                          <div class="publication_title__3m6SE">
                            Unified Implicit Neural Stylization
                          </div>
                          <div class="publication_authors__qkFXc">
                            Zhiwen Fan*†, Yifan Jiang*, Peihao Wang*, Xinyu Gong, Dejia Xu, Zhangyang Wang
                          </div>
                          <div class="publication_venue__1Dv6R">
                            ECCV 2022
                          </div>
                          <div class="publication_links__aEpO_">
                            <a href="https://arxiv.org/abs/2204.01943">Paper</a> 
                            <a href="https://zhiwenfan.github.io/INS/">Project</a> 
                            <a href="https://github.com/VITA-Group/INS">Code</a>
                          </div>
                        </div>
                      </div>
                    </div>
                  
                    <!-- Category: 3D Human -->
                    <h3 class="publication-category-header category-3d-human">3D Human</h3>
                  
                    <!-- Expressive Gaussian Human Avatars -->
                    <div class="publication-item category-3d-human">
                      <div class="publication_publication__1Icb_">
                        <video
                          class="publication_image__1EUuC"
                          src="./Homepage_files/videos/eva.mp4"
                          title="EVA video loading…"
                          playsinline autoplay loop preload muted>
                        </video>
                        <div class="publication_info__kLRGP">
                          <div class="publication_title__3m6SE">
                            Expressive Gaussian Human Avatars from Monocular RGB Video
                          </div>
                          <div class="publication_authors__qkFXc">
                            Hezhen Hu, Zhiwen Fan, Tianhao Wu, Yihan Xi, Seoyoung Lee, Georgios Pavlakos, Zhangyang Wang
                          </div>
                          <div class="publication_venue__1Dv6R">
                            NeurIPS 2024
                          </div>
                          <div class="publication_links__aEpO_">
                            <a href="https://arxiv.org/abs/2407.03204">Paper</a> 
                            <a href="https://evahuman.github.io/">Project</a> 
                            <a href="https://github.com/evahuman/EVA_Official">Code</a>
                          </div>
                        </div>
                      </div>
                    </div>
                  
                    <!-- MMHU Benchmark -->
                    <div class="publication-item category-3d-human">
                      <div class="publication_publication__1Icb_">
                        <img
                          class="publication_image__1EUuC"
                          src="./Homepage_files/MMHU.png"
                          alt="MMHU thumbnail" />
                        <div class="publication_info__kLRGP">
                          <div class="publication_title__3m6SE">
                            MMHU: A Massive‑Scale Multimodal Benchmark for Human Behavior Understanding in Autonomous Driving
                          </div>
                          <div class="publication_authors__qkFXc">
                            …, Zhiwen Fan, …
                          </div>
                          <div class="publication_venue__1Dv6R">
                            Submitted
                          </div>
                        </div>
                      </div>
                    </div>
                  
                    <!-- Category: Inverse Problems -->
                    <h3 class="publication-category-header category-inverse">Inverse Problems</h3>
                  
                    <!-- X2‑Gaussian -->
                    <div class="publication-item category-inverse">
                      <div class="publication_publication__1Icb_">
                        <video
                          class="publication_image__1EUuC"
                          src="./Homepage_files/videos/x2gs.mp4"
                          title="X2‑Gaussian video loading…"
                          playsinline autoplay loop preload muted>
                        </video>
                        <div class="publication_info__kLRGP">
                          <div class="publication_title__3m6SE">
                            X2‑Gaussian: 4D Radiative Gaussian Splatting for Continuous‑time Tomographic Reconstruction
                          </div>
                          <div class="publication_authors__qkFXc">
                            Weihao Yu, Yuanhao Cai, Ruyi Zha, Zhiwen Fan, Chenxin Li, Yixuan Yuan
                          </div>
                          <div class="publication_venue__1Dv6R">
                            Preprint
                          </div>
                          <div class="publication_links__aEpO_">
                            <a href="https://arxiv.org/abs/2407.03204">Paper</a> 
                            <a href="https://evahuman.github.io/">Project</a> 
                            <a href="https://github.com/evahuman/EVA_Official">Code</a>
                          </div>
                        </div>
                      </div>
                    </div>
                  
                    <!-- Joint CS‑MRI Reconstruction and Segmentation -->
                    <div class="publication-item category-inverse">
                      <div class="publication_publication__1Icb_">
                        <img
                          class="publication_image__1EUuC"
                          src="./Homepage_files/videos/SegMRI-ipmi.jpeg"
                          alt="Joint CS‑MRI thumbnail" />
                        <div class="publication_info__kLRGP">
                          <div class="publication_title__3m6SE">
                            Joint CS‑MRI Reconstruction and Segmentation with a Unified Deep Network
                          </div>
                          <div class="publication_authors__qkFXc">
                            Liyan Sun*, Zhiwen Fan*, Xinghao Ding, Yue Huang, John Paisley
                          </div>
                          <div class="publication_venue__1Dv6R">
                            IPMI 2019
                          </div>
                          <div class="publication_links__aEpO_">
                            <a href="https://arxiv.org/abs/1805.02165">Paper</a>
                          </div>
                        </div>
                      </div>
                    </div>
                  
                  </div>
                  


        <!-- Add CSS for filters -->
        <style>
            #pub-filters {
                margin-bottom: 20px;
                text-align: center; /* Center buttons */
            }
            .filter-btn {
                margin: 0 5px 10px; /* Add bottom margin */
                padding: 8px 15px;
                cursor: pointer;
                border: 1px solid #ccc;
                background-color: #f9f9f9;
                border-radius: 5px;
                transition: background-color 0.3s, border-color 0.3s;
            }
            .filter-btn:hover {
                background-color: #eee;
            }
            .filter-btn.active {
                background-color: #007bff; /* Example active color */
                color: white;
                border-color: #007bff;
            }
            .publication-item {
                display: block; /* Default */
                /* The original publication_publication__1Icb_ likely has margin, check if needed */
                 /* margin-bottom: 15px; */ /* Add space between items if needed*/
            }
            .publication-item.hidden {
                display: none;
            }
             .publication-category-header {
                margin-top: 30px;
                margin-bottom: 15px;
                border-bottom: 1px solid #eee;
                padding-bottom: 5px;
                font-size: 1.2em;
                font-weight: bold;
                display: block; /* Default */
             }
            .publication-category-header.hidden {
                display: none;
            }

            /* Responsive adjustments for buttons */
            @media (max-width: 768px) {
                #pub-filters {
                    text-align: left; /* Align left on smaller screens */
                }
                .filter-btn {
                     display: inline-block; /* Ensure buttons wrap */
                     margin: 3px;
                }
            }
        </style>

        <!-- Add JavaScript for filtering -->
        <script>
            function filterPublications(category) {
                // Update button active state
                const buttons = document.querySelectorAll('#pub-filters .filter-btn');
                buttons.forEach(button => {
                    button.classList.remove('active');
                    if (button.getAttribute('onclick') === `filterPublications('${category}')`) {
                        button.classList.add('active');
                    }
                });

                // Filter publications and headers
                const items = document.querySelectorAll('#publications-list .publication-item');
                const headers = document.querySelectorAll('#publications-list .publication-category-header');
                let categoryVisible = {}; // Keep track if any item in a category is visible

                 // Initialize visibility tracker by checking header classes
                headers.forEach(header => {
                    const headerCategoryClass = Array.from(header.classList).find(cls => cls.startsWith('category-'));
                    if (headerCategoryClass) {
                        categoryVisible[headerCategoryClass] = false; // Assume hidden initially
                    }
                });


                items.forEach(item => {
                    const itemCategoryClass = Array.from(item.classList).find(cls => cls.startsWith('category-'));
                    if (category === 'all' || item.classList.contains(category)) {
                        item.classList.remove('hidden');
                        if (itemCategoryClass && categoryVisible.hasOwnProperty(itemCategoryClass)) {
                            categoryVisible[itemCategoryClass] = true; // Mark category as having visible items
                        }
                    } else {
                        item.classList.add('hidden');
                    }
                });


                 // Show/hide headers based on visible items in that category or if 'all' is selected
                headers.forEach(header => {
                    const headerCategoryClass = Array.from(header.classList).find(cls => cls.startsWith('category-'));
                    // Show header if 'all' OR if its category has visible items
                    if (category === 'all' || (headerCategoryClass && categoryVisible[headerCategoryClass])) {
                        header.classList.remove('hidden');
                    } else {
                         header.classList.add('hidden');
                    }
                });
            }

            // Initial filter display (show all) when the DOM is ready
            // Run the filter function once the DOM is fully loaded to ensure elements exist
            if (document.readyState === 'loading') { // Loading hasn't finished yet
                document.addEventListener('DOMContentLoaded', () => filterPublications('all'));
            } else { // `DOMContentLoaded` has already fired
                filterPublications('all');
            }
        </script>

    </section>
    <section>
        <h2>Invited Talks</h2>
        <div style="height: 200px; overflow-y: auto; padding-right: 10px;">
            <ul>
                <li>
                    <div style="display:inline">
                        Scalable 3D/4D Assets Creation @ <strong>Duke</strong>. November 2024</span>.
                </li>

                <li>
                    <div style="display:inline">
                        E cient 3D Learning for Autonomous System @ <strong>UNC, Guest Lecture</strong>. November 2024</span>.
                    </li>

                <li>
                    <div style="display:inline">
                        Empowering Machines to Understand 3D @ <strong>Stanford, ASU, JHU, Yale</strong>. October 2024</span>.
                    </li>

                <li>
                    <div style="display:inline">
                        3D Computer Vision @ <strong>TAMU, Guest Lecture</strong>. October 2024</span>.
                    </li>

                <li>
                    <div style="display:inline">
                        From Efficient 3D Learning to 3D Foundation Models @ <strong>UCLA and CalTech</strong>. October 2024</span>.
                    </li>

                <li>
                    <div style="display:inline">
                        Towards Universal, Real-Time 3D Construction and Interaction @ <strong>TAMU AI Lunch</strong>. October 2024</span>.
                    </li>

                <li>
                    <div style="display:inline">
                        Spatial Intelligence via Reconstruction, Distillation, and Generation @ <strong>Shanghai AI Lab</strong>. July 2024</span>.
                    </li>

                <li>
                    <div style="display:inline">
                        Streamlined 3D/4D: From Hours to Seconds to Millisecond @ <strong>Google Research, <a
                            href="https://valser.org/article-761-1.html">VALSE Webinar </a></strong>. May 2024</span>.
                    </li>

                <li>
                    <div style="display:inline">
                        Streamlined 3D/4D: From Hours to Seconds to Millisecond @ <strong>Google Research</strong>. May 2024</span>.
                    </li>

                <li>
                    <div style="display:inline">
                        Real-Time Few-shot View Synthesis w/ Gaussian Splatting @ <strong>IARPA WRIVA Workshop</strong>. April 2024</span>.
                    </li>

                <li>
                    <div style="display:inline">
                        Data-efficient and Rendering-efficient Neural Rendering @ <strong>IFML Workshop on Gen AI</strong>. November 2023</span>.
                    </li>

                <li>
                    <div style="display:inline">
                        Unified Implicit Neural Stylization @ <strong>Xiamen University; Kungfu.ai.</strong> July 2022</span>.
                    </li>


            </ul>
        </div>
        <style>
            /* Custom scrollbar styling */
            div::-webkit-scrollbar {
                width: 8px;
            }

            div::-webkit-scrollbar-track {
                background: #f1f1f1;
                border-radius: 4px;
            }

            div::-webkit-scrollbar-thumb {
                background: #888;
                border-radius: 4px;
            }

            div::-webkit-scrollbar-thumb:hover {
                background: #555;
            }
        </style>
    </section>
    <section>
        <h2>Experience</h2>
        <ul>
            <li>
                <div style="display:inline">
                    Meta, Reality Lab, Burlingame<!-- -->: </div><span
                    class="styles_collaborator__VflHz">Research Intern (year of 2024)</span>.
                </li>

                <li>
                    <div style="display:inline">
                        NVIDIA Research (remote)<!-- -->: </div><span
                        class="styles_collaborator__VflHz">Research Intern (year of 2024)</span>.
                    </li>

                <li>
                    <div style="display:inline">
                        Google AR, San Francisco<!-- -->: </div><span
                        class="styles_collaborator__VflHz">Research Intern (year of 2022)</span>.
                    </li>
                <li>
                    <div style="display:inline">
                        	Alibaba Group, Hangzhou<!-- -->: </div><span
                        class="styles_collaborator__VflHz">Senior Algorithm Engineer(2019 - 2021)</span>.
                    </li>
                </ul>
            </section>
            <section>
                <h2>Services</h2>
                <ul>
                    <li>
                        <div style="display:inline">
                            Journal Reviewers:&nbsp;</div><span
                            class="styles_collaborator__VflHz">TPAMI, TIP, IJCV, Neurocomputing</span>.
                    </li>
                    <li>
                        <div style="display:inline">
                            Conference  Reviewers:&nbsp;</div><span
                            class="styles_collaborator__VflHz">NeurIPS 22/23, ICML 22/23, CVPR 22/23, ICCV 21/23, AAAI 21, ICME 2019</span>.
                    </li>
                </ul>
            </section>
            <footer class="styles_footer__3qp3V">
                <p>Last updated on <time datetime="Sep 10 2023">April, 2025</time></p>
                <p> The website template was originally borrowed from <a href="https://www.xzhou.me/">[1] </a>and <a href="https://yifanjiang19.github.io/">[2]</a>, thanks!.</p>
            </footer>
        </div>
    </div>

</body>

</html>