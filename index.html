<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>

  <title>Zhiwen Fan's Homepage</title>

  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="description" content="Zhiwen Fan is currently a senior algorithm engineer at ALibaba Cloud A.I. Labs">
  <meta name="keywords" content="Zhiwen Fan, 樊志文, Computer, Vision">
  <meta name="author" content="Zhiwen Fan" />

  <link rel="stylesheet" href="w3.css">

  <style>
  .w3-sidebar a {font-family: "Roboto", sans-serif}
  body,h1,h2,h3,h4,h5,h6,.w3-wide {font-family: "Montserrat", sans-serif;}
  </style>

  <link rel="icon" type="image/png" href="images/icon.png">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
  <!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css"> -->
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>

  <!-- Custom styles for this template -->
  <link href="css/starter-template.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href="css/theme.css" rel="stylesheet">
  <!--
  <script src="jquery.min.js"></script>
  <script>
  $(document).ready(function(){
    // Add smooth scrolling to all links
    $("a").on('click', function(event) {

      // Make sure this.hash has a value before overriding default behavior
      if (this.hash !== "") {
        // Prevent default anchor click behavior
        event.preventDefault();

        // Store hash
        var hash = this.hash;

        // Using jQuery's animate() method to add smooth page scroll
        // The optional number (800) specifies the number of milliseconds it takes to scroll to the specified area
        $('html, body').animate({
          scrollTop: $(hash).offset().top
        }, 800, function(){

          // Add hash (#) to URL when done scrolling (default click behavior)
          window.location.hash = hash;
        });
      } // End if
    });
  });
  </script>
  //-->

</head>


<body class="w3-content" style="max-width:1000px">

<!-- Sidebar/menu -->
<nav class="w3-sidebar w3-bar-block w3-black w3-collapse w3-top w3-right" style="z-index:3;width:150px" id="mySidebar">
  <div class="w3-container w3-display-container w3-padding-16">
    <h3><b>ZHIWEN</b></h3>
  </div>
  <div class="w3-padding-64 w3-text-light-grey w3-large" style="font-weight:bold">
    <a href="#home" class="w3-bar-item w3-button">Home</a>
    <a href="#news" class="w3-bar-item w3-button">News</a>
    <a href="#projects" class="w3-bar-item w3-button">Projects</a>
    <!-- <a href="#talks" class="w3-bar-item w3-button">Talks</a> -->
    <a href="#publications" class="w3-bar-item w3-button">Research</a>
    <a href="#service" class="w3-bar-item w3-button">Services</a>
    <a href="#award" class="w3-bar-item w3-button">Awards</a>
  </div>
</nav>

<!-- Top menu on small screens -->
<header class="w3-bar w3-top w3-hide-large w3-black w3-xlarge">
  <div class="w3-bar-item w3-padding-24">ZHIWEN</div>
  <a href="javascript:void(0)" class="w3-bar-item w3-button w3-padding-24 w3-right"  style="font-stretch: extra-expanded;" onclick="w3_open()"><b>≡</b></a>
  </div>
</header>

<!-- Overlay effect when opening sidebar on small screens -->
<div class="w3-overlay w3-hide-large" onclick="w3_close()" style="cursor:pointer" title="close side menu" id="myOverlay"></div>

<!-- !PAGE CONTENT! -->
<div class="w3-main" style="margin-left:150px">

  <!-- Push down content on small screens -->
  <div class="w3-hide-large" style="margin-top:83px"></div>

<!-- The Home Section -->
    <div class="w3-container w3-center w3-padding-32" id="home">
      <img style="width: 80%;max-width: 320px" alt="profile photo" src="images/selfi.jpg">
      <h1>Zhiwen Fan</h1>
        <p class="w3-justify" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:600px">
          I am a Ph.D. student in Electrical Engineering at The University of Texas at Austin advised by <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Prof. Zhangyang Wang</a> at <a href="https://vita-group.github.io/">VITA group</a>.
          </br>
          Previously, I was a senior algorithm engineer at <a href="https://ailab.aliyun.com/">Alibaba Cloud</a> worked with <a href="https://scholar.google.com/citations?user=XhyKVFMAAAAJ&hl=en">Prof. Ping Tan</a> and <a href="https://sites.google.com/site/zhusiyucs/home">Siyu Zhu</a>.
          <!-- </br> -->
          <!-- I did my M.S. at school of information, <a href="https://www.xmu.edu.cn/">Xiamen University</a> supervised by Prof. <a href="https://informatics.xmu.edu.cn/info/1011/6625.htm">Xinghao Ding</a>. -->
  <!-- where I work on deep learning and computer vision, etc. -->
        </p>
        <p class="w3-center">
          <a href="images/Zhiwen_Fan_CV.pdf">CV</a> &nbsp;/&nbsp;
          <a href="mailto:zhiwenfan@utexas.edu">Email</a> &nbsp;/&nbsp;
          <a href="https://scholar.google.com/citations?user=tdoBO3UAAAAJ&hl">Google Scholar</a> &nbsp;
          <!-- <a href="https://www.zhihu.com/people/YunheWang"> Zhi Hu </a> &nbsp;/&nbsp; -->
          <!-- <a href="https://dblp.org/pid/63/8217-1.html"> DBLP </a> -->
        </p>
  </div>

<!-- The News Section -->
<div class="w3-container w3-light-grey w3-padding-32" id="news">
  <h2>News</h2>
     <p></p><li> 07/2022, we won 3rd place of University Demo Best Demonstration at 59th Design Automation Conference <a href="https://www.dac.com/">(DAC 2022)</a>.</li><p></p>
     <p></p><li> 07/2022, three papers have been accepted by <a href="https://eccv2022.ecva.net/">ECCV 2022</a>.</li><p></p>
     <p></p><li> 05/2022, one paper has been accepted by <a href="https://icml.cc/Conferences/2022/CallForPapers">ICML 2022</a>.</li><p></p>
     <p></p><li> 03/2022, two papers have been accepted by <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>, including one oral presentation.</li><p></p>
     <p></p><li> 10/2021, one paper has been accepted by <a href="https://3dv2021.surrey.ac.uk/">3DV 2021</a>.</li><p></p>
     <p></p><li> 08/2021, the first version of floorplan CAD dataset is published at <a href="https://floorplancad.github.io/">here</a>.</li><p></p>
     <p></p><li> 07/2021, one paper has been accepted by <a href="http://iccv2021.thecvf.com/home">ICCV 2021</a>.</li><p></p>
     <p></p><li> 02/2020, one paper has been accepted by <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>, and is selected as oral presentation</li><p></p>
     <!--
     <p><li> 06/2020, two papers have been accepted by <a href="https://icml.cc/Conferences/2020/AcceptedPapersInitial">ICML 2020</a>.</li></p>
     <p><li> 07/2020, one paper has been accepted by <a href="http://2020.acmmm.org/accepted-paper-id-list.txt">ACM MM 2020</a>.</li></p>
     <p><li> 02/2020, seven papers have been accepted by <a href="http://openaccess.thecvf.com/menu.py">CVPR 2020</a>.</li></p>
     <p><li> 01/2020, one paper has been accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">IEEE TNNLS</a>.</li></p>
     <p><li> 11/2019, three papers have been accepted by <a href="https://aaai.org/Conferences/AAAI-20/wp-content/uploads/2020/01/AAAI-20-Accepted-Paper-List.pdf">AAAI 2020</a>.</li></p>
     -->
 </div>

<!-- The Projects Section -->
<div class="w3-container w3-padding-32" id="projects">
  <h2>Recent Projects</h2>
  <p class="w3-justify">
  </p>
  <h4><li>Neural Radiance Field</li></h4>
      <img style="width:30%;" src="./images/sinnerf_lego.gif">  <img style="width:30%;" src="./images/ours_lego_inter.gif">  <img style="width:30%;" src="./images/ours_room.gif">
      <figcaption>Fig1. Single-View Novel View Synthesis
           &nbsp &nbsp &nbsp  &nbsp  Fig2. NeRF Stylization &nbsp &nbsp &nbsp &nbsp &nbsp  &nbsp &nbsp &nbsp &nbsp  &nbsp &nbsp &nbsp &nbsp &nbsp  &nbsp &nbsp Fig3. NeRF Stylization</figcaption>

      <p class="w3-justify">
        Synthesizing photo-realistic images has been one of the most essential goals in the area of computer vision.
        <br>
        <p>
          <strong>NeRF Augmentations:</strong> In our <u>Aug-NeRF</u> paper, we propose to augment NeRF with worst-case perturbations in three distinct levels with physical grounds. They effectively boost NeRF in both novel view
          synthesis (up to 1.5dB PSNR gain) and underlying geometry reconstruction.
         </p>

        <p>
         <strong>Single view NeRF:</strong> In our <u>SinNeRF</u> paper, we propose thoughtfully designed semantic and geometry
         regularizations to train neural radiance field using only a single view.
        </p>

        <strong>INR Stylizatation:</strong> In our <u>INS</u> paper, we conduct a pilot study for training stylized implicit representations (e.g., SIREN, NeRF, SDF). We obtain faithful stylizations and can interpolate between different
        styles to generate new mixed style.
      </p>


      <h4><li>CAD(computer-aided design) Drawing Symbol Spotting</li></h4>
      <img style="width:96%;" src="images/floorplan_annotation.png">
      <p class="w3-justify">
      <a style="color: #447ec9" href="https://floorplancad.github.io/">Project Page</a> and <a style="color: #447ec9" href="https://www.aliyun.com/product/ai/HoloWatcher_Introduction">Product Page</a>
      </p>
      <p class="w3-justify">
        CAD symbol spotting can be use in architecture, engineering and construction (AEC) industries to accelerate the efficiency of 3D modeling from CAD drawings.
        <br>
        We release the first large-scale real-world dataset of over 10,000 CAD drawings with line-grained annotations (35 classes), covering various types of builds.
        We introduce the new task of <span style="color:red">Panoptic Symbol Spotting</span>, which is a relaxation of the traditional symbol spotting problem. It spots
        and parse both countable object instances (windows, doors, tables, etc.) and uncountable stuff (wall, railing, etc.) from CAD drawings,
        Moreover, we propose the Panoptic Quality (PQ) as the evaluation criteria of panotic symbol spotting results.
        <br>
        We present a CNN-GCN method in our <a href=https://arxiv.org/abs/2105.07147><u>ICCV2021</u></a> which unified a GCN head and a detection head for semantic and instance symbol spotting respectively.
        Then, we present a transformer-based framework named <a href=https://openaccess.thecvf.com/content/CVPR2022/papers/Fan_CADTransformer_Panoptic_Symbol_Spotting_Transformer_for_CAD_Drawings_CVPR_2022_paper.pdf><u>CADTransormer (CVPR2022)</u></a>,
        by painlessly modifying existing vision transformer (ViT)
        backbones.
      </p>

      <h4><li>Efficient Multi-view Stereo and Stereo Matching</li></h4>
      <img style="width:96%;" src="images/cascade_costvolume_teaser.jpg">
      <p class="w3-justify">
      <!-- <a style="color: #447ec9" href="https://live.huawei.com/huaweiconnect/meeting/cn/5872.html">Huawei Connect (HC) 2020</a> | <a style="color: #447ec9" href="https://www.mindspore.cn/resources/hub">MindSpore Hub</a> -->
      </p>
      <p class="w3-justify">
        To tackle the high computaitonal cost of the existing cost volume-based deep MVS and stereo matching methods, we propose Cas-MVSNet and Cas-StereoNet.
        By formulating cost volume in a coarse to fine manner, we obtain a <span style="color:red">23.1%</span> improvement on
        <a href=http://roboimagedata.compute.dtu.dk/?page_id=36>DTU dataset</a>  with <span style="color:red">50.6%</span> and <span style="color:red">74.2%</span> reduction in GPU memory and run-time.
        It is also rank 1st within all learning-based methods on <a href=https://www.tanksandtemples.org/>Tanks and Temples benchmark</a>.
        See<a href=https://arxiv.org/abs/1912.06378> CVPR2020</a> for more details.
      </p>

      <h4><li>Compressed Sensing MRI and Image Deraining</li></h4>
      <p class="w3-justify">
      <!-- <a style="color: #447ec9" href="https://live.huawei.com/huaweiconnect/meeting/cn/5872.html">Huawei Connect (HC) 2020</a> | <a style="color: #447ec9" href="https://www.mindspore.cn/resources/hub">MindSpore Hub</a> -->
      </p>
      <p class="w3-justify">
        I have also worked on low-level computer vision tasks (e.g. Compressed Sensing MRI and Single Image Deraining) using Deep Neural Network before the year 2019.
        See <a style="color: #447ec9" href="https://arxiv.org/abs/1805.02165">IPMI2019</a>,
        <a style="color: #447ec9" href="https://arxiv.org/abs/1804.07493">ACM MM2019</a>,
        <a style="color: #447ec9" href="https://openaccess.thecvf.com/content_ECCV_2018/html/Zhiwen_Fan_A_Segmentation-aware_Deep_ECCV_2018_paper.html">ECCV 2018</a>,
        <a style="color: #447ec9" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16546">AAAI 2018</a> and
        <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/8758456/">TIP 2019</a>,
        <a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/abs/pii/S0730725X19301870">MRI 2019</a>,
        <a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/abs/pii/S0730725X1830674X">MRI 2019</a> for details.
      </p>

</div>


  <!-- The Talks Section -->
  <!-- <div class="w3-container w3-light-grey w3-padding-32" id="talks">
    <h2>Talks</h2>
      <p><li> 06/2020, "<a href="http://valser.org/webinar/slide/slides/20200603/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9-%E5%B7%A5%E4%B8%9A%E7%95%8C%E5%92%8C%E5%AD%A6%E6%9C%AF%E7%95%8C%E7%9A%84%E5%B7%AE%E5%BC%82.pdf">AI on the Edge - Discussion on the Gap Between Industry and Academia</a>" at <a  href="http://valser.org/"><strong>VALSE</strong></a> Webinar.</li></p>
      <p><li> 05/2020, "<a href="https://www.bilibili.com/video/av925692420/"> Edge AI: Progress and Future Directions</a>" at <a href="https://www.qbitai.com/"> <strong>QbitAI</strong></a> using <a  href="https://www.bilibili.com/"><strong>bilibili</strong></a>.</li></p>
  </div> -->

 <!-- The Publications Section -->
  <div class="w3-container w3-padding-32"" id="publications">
    <h2>Publications</h2>
      <p class="w3-left-align" style="line-height:200%">
        I'm interested in devleoping <strong>Implicit Neural Representations</strong>, <strong>Efficient 3D Reconstruction</strong>, <strong>Transformer Models</strong> and <strong>Low-level Computer Vision</strong>.
      </p>

    <!-- <h4> Arxiv Submissions:</h4> -->


    <h4> Conference Papers:</h4>

    <ol>
      <p>
        <li><strong>Unified Implicit Neural Stylization</strong>
        <br>
        <strong>Zhiwen Fan*</strong>, Yifan Jiang*, Peihao Wang*, Xinyu Gong, Dejia Xu, Zhangyang Wang.
        <br>
        <em>ECCV</em> 2022| <a style="color: #447ec9" href="https://arxiv.org/abs/2204.01943">paper</a>  | <a style="color: #447ec9" href="https://zhiwenfan.github.io/INS/">project page</a>| <a style="color: #447ec9" href="https://github.com/VITA-Group/INS">code</a>
      </p>

      <p>
        <li><strong>SinNeRF: Training Neural Radiance Fields on Complex Scenes from a Single Image</strong>
        <br>
       Dejia Xu*, Yifan Jiang*, Peihao Wang, <strong>Zhiwen Fan</strong>, Humphrey Shi, Zhangyang Wang.
        <br>
        <em>ECCV</em> 2022| <a style="color: #447ec9" href="https://arxiv.org/abs/2204.00928">paper</a>  | <a style="color: #447ec9" href="https://vita-group.github.io/SinNeRF/">project page</a>| <a style="color: #447ec9" href="https://github.com/VITA-Group/SinNeRF">code</a>
      </p>

    <p>
      <li><strong>Point Cloud Domain Adaptation via Masked Local 3D Structure Prediction</strong>
      <br>
     Hanxue Liang , Hehe Fan, <strong>Zhiwen Fan</strong>, Yi Wang, Tianlong Chen, Yu Cheng, Zhangyang Wang.
      <br>
      <em>ECCV</em> 2022| <a style="color: #447ec9" href="https://arxiv.org/abs/TBD">paper</a>  | <a style="color: #447ec9" href="https://github.com/VITA-Group/MLSP">code</a>
    </p>

      <p>
        <li><strong>Neural Implicit Dictionary Learning via Mixture-of-Expert Training</strong>
        <br>
       Peihao Wang, <strong>Zhiwen Fan</strong>, Tianlong Chen, Zhangyang Wang.
        <br>
        <em>ICML</em> 2022| <a style="color: #447ec9" href="https://arxiv.org/abs/tbd">paper</a>  |  <a style="color: #447ec9" href="https://github.com/VITA-Group/tbd">code</a>
      </p>

      <p>
        <li><strong>CADTransformer: Panoptic Symbol Spotting Transformer for CAD Drawings</strong>
        <br>
        <strong>Zhiwen Fan</strong>, Tianlong Chen, Peihao Wang, Zhangyang Wang
        <br>
        <em>CVPR</em> 2022 <strong> (Oral Presentation) </strong> | <a style="color: #447ec9" href="https://openaccess.thecvf.com/content/CVPR2022/html/Fan_CADTransformer_Panoptic_Symbol_Spotting_Transformer_for_CAD_Drawings_CVPR_2022_paper.html">paper</a> | <a style="color: #447ec9" href="https://github.com/VITA-Group/CADTransformer">code</a>
      </p>
      <p>
        <li><strong>Aug-NeRF: Training Stronger Neural Radiance Fields with Triple-Level Augmentations</strong>
        <br>
       Tianlong Chen, Peihao Wang, <strong>Zhiwen Fan</strong>, Zhangyang Wang
        <br>
        <em>CVPR</em> 2022 | <a style="color: #447ec9" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Aug-NeRF_Training_Stronger_Neural_Radiance_Fields_With_Triple-Level_Physically-Grounded_Augmentations_CVPR_2022_paper.pdf">paper</a> | <a style="color: #447ec9" href="https://github.com/VITA-Group/Aug-NeRF">code</a>
      </p>

      <p>
        <li><strong>MeshMVS: Multi-View Stereo Guided Mesh Reconstruction</strong>
        <br>
       Rakesh Shrestha, <strong>Zhiwen Fan</strong>, Qingkun Su, Zuozhuo Dai, Siyu Zhu, Ping Tan
        <br>
        <em>3DV</em> 2021 | <a style="color: #447ec9" href="https://arxiv.org/abs/2010.08682">paper</a> | <a style="color: #447ec9" href="https://github.com/rakeshshrestha31/meshmvs">code</a>
      </p>
      <p>
      <li><strong>FloorPlanCAD: A Large-Scale CAD Drawing Dataset for Panoptic Symbol Spotting</strong>
      <br>
      <strong>Zhiwen Fan*</strong>,Lingjie Zhu*, Honghua Li, Xiaohao Chen, Siyu Zhu, Ping Tan
      <br>
      <em>ICCV</em> 2021 | <a style="color: #447ec9" href="https://arxiv.org/abs/2105.07147">paper</a> | <a style="color: #447ec9" href="images/floorplancad_poster.pdf">poster</a> | <a style="color: #447ec9" href="https://www.youtube.com/watch?v=bAW0mjVH7us">video</a>
      </p>

      <p>
      <li><strong>Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching</strong>
      <br>
      <strong>Zhiwen Fan*</strong>, Xiaodong Gu*, Siyu Zhu, Zuozhuo Dai, Feitong Tan, Ping Tan
      <br>
      <em>CVPR</em> 2020 <strong> (Oral Presentation) </strong> | <a style="color: #447ec9" href="https://arxiv.org/abs/1912.06378">paper</a> | <a style="color: #447ec9" href="https://github.com/alibaba/cascade-stereo">code</a> | <a style="color: #447ec9" href="https://www.youtube.com/watch?v=rcJiRQqDKbo">video</a>
      </p>

      <p>
      <li><strong>Joint CS-MRI reconstruction and segmentation with a unified deep network</strong>
      <br>
      <strong>Zhiwen Fan*</strong>,Liyan Sun*, Xinghao Ding, Yue Huang, John Paisley
      <br>
      <em>IPMI</em> 2019 | <a style="color: #447ec9" href="https://arxiv.org/abs/1805.02165">paper</a>
      </p>

      <p>
      <li><strong>Residual-guide network for single image deraining</strong>
      <br>
      <strong>Zhiwen Fan*</strong>, Huafeng Wu*, Xueyang Fu, Yue Huang, Xinghao Ding
      <br>
      <em>ACM MM</em> 2019 | <a style="color: #447ec9" href="https://arxiv.org/abs/1804.07493">paper</a>
      </p>

      <p>
      <li><strong>A Segmentation-aware Deep Fusion Network for Compressed Sensing MRI</strong>
      <br>
      <strong>Zhiwen Fan*</strong>, Liyan Sun*, Xinghao Ding, Yue Huang, Congbo Cai, John Paisley
      <br>
      <em>ECCV</em> 2018 | <a style="color: #447ec9" href="https://openaccess.thecvf.com/content_ECCV_2018/html/Zhiwen_Fan_A_Segmentation-aware_Deep_ECCV_2018_paper.html">paper</a>
      </p>

      <p>
      <li><strong>Compressed Sensing MRI Using a Recursive Dilated Network</strong>
      <br>
      <strong>Zhiwen Fan*</strong>, Liyan Sun*, Yue Huang, Xinghao Ding, John Paisley
      <br>
      <em>AAAI</em> 2018 (* equal contribution) | <a style="color: #447ec9" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16546">paper</a>
      </p>

      <p>
      <li><strong>Two-step approach for single underwater image enhancement</strong>
      <br>
     Xueyang Fu, <strong>Zhiwen Fan</strong>, Mei Ling, Yue Huang, Xinghao Ding
      <br>
      <em>ISPACS</em> 2017 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/8266583/">paper</a>
      </p>
      </ol>

      <h4> Journal Papers:</h4>

      <ol>

      <p>
      <li><strong>A deep information sharing network for multi-contrast compressed sensing MRI reconstruction</strong>
      <br>
     Liyan Sun, <strong>Zhiwen Fan</strong>, Xueyang Fu, Yue Huang, Xinghao Ding, John Paisley
      <br>
      <em>IEEE TIP</em> 2019 |<a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/8758456/">paper</a>
      </p>

      <p>
      <li><strong>Region-of-interest undersampled MRI reconstruction: A deep convolutional neural network approach</strong>
      <br>
     Liyan Sun, <strong>Zhiwen Fan</strong>, Xinghao Ding, Yue Huang, John Paisley
      <br>
      <em>Magnetic resonance imaging</em> 2019 | <a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/abs/pii/S0730725X19301870">paper</a>
      </p>

      <p>
      <li><strong>A divide-and-conquer approach to compressed sensing MRI</strong>
      <br>
     Liyan Sun, <strong>Zhiwen Fan</strong>, Xinghao Ding, Congbo Cai, Yue Huang, John Paisley
      <br>
      <em>Magnetic resonance imaging</em> 2019 | <a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/abs/pii/S0730725X1830674X">paper</a>
      </p>
    </ol>

      <!-- <h4> Arxiv Papers:</h4>
      <ol>
      <p>
        <li><strong>MeshMVS: Multi-View Stereo Guided Mesh Reconstruction</strong>
        <br>
        Rakesh Shrestha, Zhiwen Fan, Qingkun Su, Zuozhuo Dai, Siyu Zhu, Ping Tan
        <br>
        <em>arXiv preprint</em> 2021 | <a style="color: #447ec9" href="https://arxiv.org/abs/2010.08682">paper</a> | <a style="color: #447ec9" href="https://github.com/rakeshshrestha31/meshmvs">code</a>
        </p>
      </ol>
       -->

    </p>
  </div>

  <div class="w3-container w3-padding-32" id="service">
    <h2>Experience</h2>

    <div class="media">
      <div class="media-left">
          <img class="media-object" style="width:64px;" src="./images/google-1.jpg" alt="GOOGLE">
        </a>
      </div>
      <div class="media-body">
        <h4 class="media-heading">Google, San Francisco, US</h4>
        <p>Research Intern<br>05/2022--present</p>
      </div>

      <div class="media">
        <div class="media-left">
            <img class="media-object" style="width:64px;" src="./images/alibaba.png" alt="aliabab">
          </a>
        </div>
        <div class="media-body">
          <h4 class="media-heading">Alibaba Cloud, Hangzhou, China</h4>
          <p>Senior Algorithm Engineer<br>07/2019--08/2021</p>
        </div>


  </div>
  </div>

<!-- The Services Section -->
  <div class="w3-container  w3-padding-32" id="service">
    <h2>Services</h2>
      <!-- <p><li> Area Chair of <a href="https://icml.cc/Conferences/2021">ICML 2021</a>, <a href="https://nips.cc/Conferences/2021/">NeurIPS 2021</a>.</p> -->
      <!-- <p><li> Senior Program Committee Members of <a href="https://ijcai-21.org/">IJCAI 2021</a>, <a href="https://www.ijcai20.org/">IJCAI 2020</a> and <a href="https://www.ijcai19.org/program-committee.html">IJCAI 2019</a>.</p> -->
      <p><li> Journal Reviewers of IJCV, Neurocomputing</p>
      <p><li> Conference Reviewers of ICML 2022, CVPR 2022, ICCV 2021, AAAI 2021, ICME 2019</p>
  </div>


  <div class="w3-container  w3-padding-32" id="service">
    <h2>Invited Talk</h2>
      <!-- <p><li> Area Chair of <a href="https://icml.cc/Conferences/2021">ICML 2021</a>, <a href="https://nips.cc/Conferences/2021/">NeurIPS 2021</a>.</p> -->
      <!-- <p><li> Senior Program Committee Members of <a href="https://ijcai-21.org/">IJCAI 2021</a>, <a href="https://www.ijcai20.org/">IJCAI 2020</a> and <a href="https://www.ijcai19.org/program-committee.html">IJCAI 2019</a>.</p> -->
      <p><li> [June 2022] "Unified Implicit Neural Stylization" at Xiamen University and Kungfu.ai. <a href="images/INS.pdf">Slides</a></p>
  </div>

  <!-- The Awards Section -->
  <div class="w3-container w3-padding-32" id="award">
    <h2>Awards</h2>
    <p><li> 2022, 3rd place of University Demo Best Demonstration at 59th Design Automation Conference</a></p>
    <p><li> 2022, Professional Development Award of UT Austin</a></p>
    <p><li> 2019, Outstanding Graduates of Xiamen University</a></p>
    <p><li> 2017 The First Prize Scholarship of Xiamen University</p>
    <p><li> 2016 The First Prize Scholarship of Xiamen University</p>
    <p><li> 2016, Outstanding Graduates of Shandong Provience</p>
  </div>

  <div class="w3-light-grey w3-center w3-padding-24">

  </div>

  <!-- End page content -->
</div>

<script>
// Accordion
function myAccFunc() {
  var x = document.getElementById("demoAcc");
  if (x.className.indexOf("w3-show") == -1) {
    x.className += " w3-show";
  } else {
    x.className = x.className.replace(" w3-show", "");
  }
}

// Click on the "Jeans" link on page load to open the accordion for demo purposes
document.getElementById("myBtn").click();


// Open and close sidebar
function w3_open() {
  document.getElementById("mySidebar").style.display = "block";
  document.getElementById("myOverlay").style.display = "block";
}

function w3_close() {
  document.getElementById("mySidebar").style.display = "none";
  document.getElementById("myOverlay").style.display = "none";
}
</script>

</body>
</html>
