<!DOCTYPE html>
<!-- saved from url=(0017)https://xzhou.me/ -->
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><!--<base target="_blank">-->
    <base href="." target="_blank">
    <link rel="preconnect" href="https://fonts.gstatic.com/">
    <link rel="stylesheet" type="text/css" data-href="https://fonts.googleapis.com/css?family=Lato">
    <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin="">
    <meta name="viewport" content="width=device-width">
    <title>Zhiwen Fan's Homepage</title>
    <meta name="description" content="I am a Ph.D. student in Electrical Computer Engineering at The University of Texas at Austin">
    <meta name="next-head-count" content="4">
    <link rel="preload" href="./Homepage_files/fd4c9bf86266e80fae27.css" as="style">
    <link rel="stylesheet" href="./Homepage_files/fd4c9bf86266e80fae27.css" data-n-g="">
    <link rel="preload" href="./Homepage_files/a515be4c2f49419f3769.css" as="style">
    <link rel="stylesheet" href="./Homepage_files/a515be4c2f49419f3769.css" data-n-p=""><noscript
        data-n-css=""></noscript>
    <script defer="" nomodule="" src="./Homepage_files/polyfills-a54b4f32bdc1ef890ddd.js.下载"></script>
    <script src="./Homepage_files/webpack-715970c8028b8d8e1f64.js.下载" defer=""></script>
    <script src="./Homepage_files/framework-64eb7138163e04c228e4.js.下载" defer=""></script>
    <script src="./Homepage_files/main-c94e7f60255631414010.js.下载" defer=""></script>
    <script src="./Homepage_files/_app-3fa27215a3c41987e6d2.js.下载" defer=""></script>
    <script src="./Homepage_files/688-b942890a87f9a08b0e31.js.下载" defer=""></script>
    <script src="./Homepage_files/index-335648998a7bdac0c719.js.下载" defer=""></script>
    <script src="./Homepage_files/_buildManifest.js.下载" defer=""></script>
    <script src="./Homepage_files/_ssgManifest.js.下载" defer=""></script>
    <style data-href="https://fonts.googleapis.com/css?family=Lato">
        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 400;
            src: url(https://fonts.gstatic.com/s/lato/v24/S6uyw4BMUTPHjx4wWA.woff) format('woff')
        }

        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 400;
            src: url(https://fonts.gstatic.com/s/lato/v24/S6uyw4BMUTPHjxAwXiWtFCfQ7A.woff2) format('woff2');
            unicode-range: U+0100-02AF, U+0304, U+0308, U+0329, U+1E00-1E9F, U+1EF2-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF
        }

        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 400;
            src: url(https://fonts.gstatic.com/s/lato/v24/S6uyw4BMUTPHjx4wXiWtFCc.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD
        }
    </style>
</head>

<body>

    <div id="__next">
        <div class="styles_container__1_WuM">
            <section>
                <div class="personal_profile__BdQI4"><img class="personal_portrait__BAkO0"
                        src="./Homepage_files/zhiwenfan_square.JPG" alt="pottrait">
                    <div class="personal_profileInfo__1Y4pW">
                        <h2 class="personal_name__1_mHK">Zhiwen ("Aaron") Fan</h2>
                        <!-- <h3 class="personal_chineseName__18aOD">樊志文</h3> -->
                        <h3 class="personal_worksFor__2L0De">University of Texas at Austin</h3>
                        <div class="personal_links__p_eYL"><span><a
                                    href="mailto:zhiwenfan@utexas.edu">Email</a></span>
                                    <span><a href="https://scholar.google.com.hk/citations?user=tdoBO3UAAAAJ&hl=en-us">Google Scholar</a></span>
                                    <span><a href="./Homepage_files/Zhiwen_Fan_UTAustin_CV.pdf">CV</a></span>
                                    <span><a href="./Homepage_files/Zhiwen_Fan_UTAustin_ResearchStatement.pdf">Research Statement</a></span>
                                </div>
                    </div>
                </div>
            </section>
            <section>
                <h2>About Me</h2>
                <div>
                    <p>I am a Ph.D. candidate in Electrical Computer Engineering at The University of Texas at Austin advised by <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang">Prof. Atlas Wang</a> at <a href="https://vita-group.github.io/">VITA group</a>. <br>
                        I work closely with <a href="https://profiles.stanford.edu/marco-pavone">Prof. Marco Pavone</a> and <a href="https://yuewang.xyz/">Prof. Yue Wang</a> on 3D end-to-end models with robust generalization capabilities; with <a href="https://www.ee.ucla.edu/achuta-kadambi/">Prof. Achuta Kadambi</a> on recovering 3D/4D signals that capture the space-time structure of our world from casually captured data; and with <a href="https://ece.gatech.edu/directory/callie-hao">Prof. Callie Hao</a> on hardware-software co-design.
                        I was the awardees of <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2022-north-america">Qualcomm Innovation Fellowship 2022</a>.
                    </p>


                </div>
            </section>
            <section>
                <section>
                <h2>Recent News</h2>
                <div style="height: 200px; overflow-y: auto; padding-right: 10px;">
                    <ul>
                        <li>I will serve as the Area Chair for NeurIPS 2025.</li>
                        <li>Our ICLR'25 (<a href="https://4k4dgen.github.io/">4K4DGen</a>) is selected as <strong>spotlight</strong> presentation. </li>
                        <li>Our NeurIPS'24 (<a href="https://lightgaussian.github.io/">LightGaussian</a>) is selected as <strong>spotlight</strong> presentation. </li>
                        <li>Our <a href="https://arxiv.org/abs/2212.14849">Symbolic Visual RL</a> was accepted by IEEE Trans. PAMI. </li>
                        <li>Our IROS'24 (<a href="https://arxiv.org/abs/2404.00923">Multi-modal 3DGS SLAM</a>)
                            is selected as <strong>oral pitch finalist</strong> presentation. </li>
                        <li>Our CVPR'24 (<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Feature_3DGS_Supercharging_3D_Gaussian_Splatting_to_Enable_Distilled_Feature_CVPR_2024_paper.pdf">Feature-3DGS</a>) is selected as <strong>highlight</strong> presentation. </li>
                        <li>Our CVPR'23 (<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_NeuralLift-360_Lifting_an_In-the-Wild_2D_Photo_to_a_3D_Object_CVPR_2023_paper.pdf">NeuralLift-360</a>) is selected as <strong>highlight</strong> presentation. </li>
                        <li>I was one of the awardees of the <strong><span style="color: darkblue;">Qualcomm Innovation Fellowship</span></strong> (North America) 2022 <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2022-north-america">(QIF 2022)</a>. Innovation title: "Real-time Visual Processing for Autonomous Driving via Video Transformer with Data-Model-Accelerator Tri-design". </li>
                        <li>We won 3rd place in the <strong><span style="color: darkblue;">University Demo Best Demonstration</span></strong> at the 59th Design Automation Conference <a href="https://www.dac.com/">(DAC 2022)</a>. We demo for a multi-task vision transformer on FPGA. </li>
                        <li>Our CVPR'22 (<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fan_CADTransformer_Panoptic_Symbol_Spotting_Transformer_for_CAD_Drawings_CVPR_2022_paper.pdf">CADTransformer</a>) is selected as <strong>oral</strong> presentation. </li>
                        <li>Our paper for CVPR'20 (<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Gu_Cascade_Cost_Volume_for_High-Resolution_Multi-View_Stereo_and_Stereo_Matching_CVPR_2020_paper.pdf">Cascade Cost Volume</a>) is selected as <strong>oral</strong> presentation. </li>
                    </ul>
                </div>
            </section>

            <style>
                /* Custom scrollbar styling */
                div::-webkit-scrollbar {
                    width: 8px;
                }

                div::-webkit-scrollbar-track {
                    background: #f1f1f1;
                    border-radius: 4px;
                }

                div::-webkit-scrollbar-thumb {
                    background: #888;
                    border-radius: 4px;
                }

                div::-webkit-scrollbar-thumb:hover {
                    background: #555;
                }
            </style>


            <section>
                <h2>Researches Interests<br class="publication-list_mobileBreak__24vsO"></h2>


                <div class="publication_publication__1Icb_">

                    <div class="publication_image_0dot9_size"><img src="./Homepage_files/research_overview.svg"
                        alt="loading...">
                </div>
            </div>
            My research goal is to enhance AI agents' ability to interact with the physical world through 3D AI.
            I focus on developing multi-modal 3D models that enable perception, generation, and action in 3D space.
            By integrating natural language, images, videos, and 3D data, my work aims to bridge AI agents,
            human instructions, and the physical world. I am particularly interested in building real-time 3D models
            that can understand, recreate, and interact with their environment using spatial awareness and common sense.


                <div>
                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__full_size"><video src="./Homepage_files/videos/homepage_applications.mp4"
                            title="Video demos for application is loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video>
                    </div>
                </div>
                <p>My research has been demonstrated on platforms such as Quest 3, implemented within IARPA projects, and integrated into multiple commercial products.</p>


            </section>

            <section>
                <h2>Selected Publications<br class="publication-list_mobileBreak__24vsO"><span
                        class="publication-list_filters__3ikvu"><span>Full publication list at </span><span><a
                                href="https://scholar.google.com.hk/citations?hl=en&user=tdoBO3UAAAAJ&view_op=list_works&sortby=pubdate">Google
                                Scholar</a></span></span></h2>
                <div class="publication-list_smallText__pUJXB">* denotes equal contribution, &dagger; denotes project lead.</div>
                <div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/lsm.mp4"
                            title="Large Spatial Model video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Large Spatial Model: Real-time Unposed Images to Semantic 3D</div>
                            <div class="publication_authors__qkFXc"><span>Zhiwen Fan* <sup>&dagger;</sup>, Jian Zhang*, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi,  Zhangyang Wang, Danfei Xu, Boris Ivanovic, Marco Pavone, Yue Wang </span></div>
                            <div class="publication_venue__1Dv6R"><span>NeurIPS 2024</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://openreview.net/forum?id=ybHPzL7eYT&referrer=%5Bthe%20profile%20of%20Yue%20Wang%5D(%2Fprofile%3Fid%3D~Yue_Wang2)">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://largespatialmodel.github.io/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/NVlabs/LSM">Code</a></span></div>
                        </div>
                    </div>


                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/4k4dgen.mp4"
                            title="4K4DGen video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">4K4DGen: Panoramic 4D Generation at 4K Resolution</div>
                            <div class="publication_authors__qkFXc"><span>Renjie Li, Panwang Pan, Dejia Xu, Shijie Zhou, Xuanyang Zhang, Zeming Li, Achuta Kadambi, Zhangyang Wang, Zhiwen Fan </span></div>
                            <div class="publication_venue__1Dv6R"><span>ICLR 2025</span><span class="publication_highlights__2ILmf">(Spotlight, 3.2% among 11672)</span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://openreview.net/forum?id=qxRoo7ULCo">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://4k4dgen.github.io/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://4k4dgen.github.io/">Code</a></span></div>
                        </div>
                    </div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/garden_lighgaussian.mp4"
                            title="LightGaussian video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</div>
                            <div class="publication_authors__qkFXc"><span><span>Zhiwen</span>
                                    <span>Fan*<sup>&dagger;</sup></span></span><span><span>Kevin</span>
                                    <span>Wang*</span></span><span><span>Kairun </span>
                                    <span>Wen</span></span><span><span>Zehao </span>
                                    <span>Zhu</span></span><span><span>Dejia </span>
                                    <span>Xu</span></span><span><span>Zhangyang </span>
                                    <span>Wang</span></div>
                            <div class="publication_venue__1Dv6R"><span>NeurIPS 2024</span><span class="publication_highlights__2ILmf">(Spotlight, top 2.1% among 15671)</span>&nbsp;&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/fd881d3b625437354d4421818f81058f-Abstract-Conference.html">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://lightgaussian.github.io/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VITA-Group/LightGaussian">Code</a></span></div>
                        </div>
                    </div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/moonsim.mp4"
                            title="Moonsim video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">MoonSim: A Photorealistic Lunar Environment Simulator </div>
                            <div class="publication_authors__qkFXc"><span>Ziyu Chen*, Henghui Bao*, Ting-Hsuan Chen*, Haozhe Lou, Ge Yang, Zhiwen Fan, Marco Pavone, Yue Wang </span></div>
                            <div class="publication_venue__1Dv6R"><span>under submission</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://anonymousi079j.github.io/moonsim/">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://anonymousi079j.github.io/moonsim/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://anonymousi079j.github.io/moonsim/">Code</a></span></div>
                        </div>
                    </div>



                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/eva.mp4"
                            title="Large Spatial Model video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Expressive Gaussian Human Avatars from Monocular RGB Video</div>
                            <div class="publication_authors__qkFXc"><span>Hezhen Hu, Zhiwen Fan, Tianhao Wu, Yihan Xi, Seoyoung Lee, Georgios Pavlakos, Zhangyang Wang </span></div>
                            <div class="publication_venue__1Dv6R"><span>NeurIPS 2024</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/2407.03204">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://evahuman.github.io/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/evahuman/EVA_Official">Code</a></span></div>
                        </div>
                    </div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/mmgsslam.mp4"
                            title="MM3DGS-SLAM video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements </div>
                            <div class="publication_authors__qkFXc"><span>Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu </span></div>
                            <div class="publication_venue__1Dv6R"><span>IROS 2024 <span class="publication_highlights__2ILmf">(Oral Pitch Highlight)</span></span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/2404.00923">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://vita-group.github.io/MM3DGS-SLAM/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VITA-Group/MM3DGS-SLAM">Code</a></span></div>
                        </div>
                    </div>



                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/instantsplat.mp4"
                            title="Instantsplat video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">InstantSplat: Sparse-view Pose-free Gaussian Splatting in Seconds</div>
                            <div class="publication_authors__qkFXc"><span>Zhiwen Fan*<sup>&dagger;</sup>, Wenyan Cong*, Kairun Wen*, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, Zhangyang Wang, Yue Wang </span></div>
                            <div class="publication_venue__1Dv6R"><span>Preprint</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/2403.20309">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://instantsplat.github.io/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/NVlabs/InstantSplat">Code</a></span></div>
                        </div>
                    </div>



                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/dreamscene360.mp4"
                            title="dreamscene360 video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting</div>
                            <div class="publication_authors__qkFXc"><span>Shijie Zhou*, Zhiwen Fan*, Dejia Xu*, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi </span></div>
                            <div class="publication_venue__1Dv6R"><span>ECCV 2024</span><span></span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/2404.06903">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://dreamscene360.github.io/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://dreamscene360.github.io/">Code</a></span></div>
                        </div>
                    </div>


                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/feature-3dgs.mp4"
                            title="Feature 3DGS video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields</div>
                            <div class="publication_authors__qkFXc">
                                    Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, Achuta Kadambi,</span>
                                    </div>
                            <div class="publication_venue__1Dv6R"><span>CVPR 2024</span><span class="publication_highlights__2ILmf">(Highlight, 2.8% of 11532)</span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/2312.03203">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://feature-3dgs.github.io/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/ShijieZhou-UCLA/feature-3dgs">Code</a></span></div>
                        </div>
                    </div>


                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/nerf_sos.mp4"
                            title="NeRF-SOS video loading.." playsinline="" autoplay="" loop="" preload=""
                            muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">NeRF-SOS: Any-View Self-supervised Object Segmentation from Complex Real-World Scenes</div>
                            <div class="publication_authors__qkFXc"><span><span>Zhiwen</span>
                                    <span>Fan</span></span><span><span>Peihao</span>
                                    <span>Wang</span></span><span><span>Yifan</span>
                                    <span>Jiang</span></span><span><span>Xinyu </span>
                                    <span>Gong</span></span><span><span>Dejia </span>
                                    <span>Xu</span></span><span><span>Zhangyang </span>
                                    <span>Wang</span></div>
                            <div class="publication_venue__1Dv6R"><span>ICLR</span><span>2023</span>&nbsp;</div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://openreview.net/pdf?id=kfOtMqYJlUU">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://zhiwenfan.github.io/NeRF-SOS/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VITA-Group/NeRF-SOS">Code</a></span></div>
                        </div>
                    </div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><video src="./Homepage_files/videos/INS_merge.mp4"
                                title="INS video loading.." playsinline="" autoplay="" loop="" preload=""
                                muted=""></video></div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Unified Implicit Neural Stylization</div>
                            <div class="publication_authors__qkFXc"><span><span>Zhiwen</span>
                                    <span>Fan*<sup>&dagger;</sup></span></span><span><span>Yifan</span>
                                    <span>Jiang*</span></span><span><span>Peihao</span>
                                    <span>Wang*</span></span><span><span>Xinyu </span>
                                    <span>Gong</span></span><span><span>Dejia</span>
                                    <span>Xu</span></span><span><span>Zhangyang</span> <span>Wang</span></span></div>
                            <div class="publication_venue__1Dv6R"><span>ECCV</span><span>2022</span>&nbsp; </div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/2204.01943">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://zhiwenfan.github.io/INS/">Project</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VITA-Group/INS">Code</a></span></div>
                        </div>
                    </div>
                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><img src="./Homepage_files/videos/m3vit.png"
                                alt="M^3ViT Video thumbnail loading...">
                        </div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">M^3ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design</div>
                            <div class="publication_authors__qkFXc"><span><span>Zhiwen </span>
                                    <span>Fan*<sup>&dagger;</sup></span></span><span><span>Hanxue </span>
                                    <span>Liang*</span></span><span><span>Rishov </span>
                                    <span>Sarkar</span></span><span><span>Ziyu </span>
                                    <span>Jiang</span></span><span><span>Tianlong </span>
                                    <span>Chen</span></span><span><span>Kai </span>
                                    <span>Zou</span></span><span><span>Yu </span>
                                    <span>Cheng</span></span><span><span>Cong  </span>
                                    <span>Hao</span></span><span><span>Zhangyang   </span>
                                    <span>Wang</span>
                                </div>
                            <div class="publication_venue__1Dv6R"><span>NeurIPS</span><span>2022</span>&nbsp;
                                <span
                                class="publication_highlights__2ILmf">(QIF 2022 Award & DAC 3rd best demo)</span>
                            </div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/pdf/2210.14793">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VITA-Group/M3ViT">Code</a></span></div>
                        </div>
                    </div>

                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><img src="./Homepage_files/videos/floorplancad.png"
                            alt="CADTransformer...">
                    </div>
                        <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">CADTransformer: Panoptic Symbol Spotting Transformer for CAD Drawings</div>
                            <div class="publication_authors__qkFXc"><span><span>Zhiwen </span>
                                    <span>Fan</span></span><span><span>Tianlong </span>
                                    <span>Chen</span></span><span><span>Peihao </span>
                                    <span>Wang</span></span><span><span>Zhangyang </span>
                                    <span>Wang</span></div>
                            <div class="publication_venue__1Dv6R"><span>CVPR</span><span>2022</span>&nbsp;<span
                                class="publication_highlights__2ILmf">(Oral Presentation, top 5% in all submissions)</span> </div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://openaccess.thecvf.com/content/CVPR2022/html/Fan_CADTransformer_Panoptic_Symbol_Spotting_Transformer_for_CAD_Drawings_CVPR_2022_paper.html">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/VITA-Group/CADTransformer">Code</a></span></div>
                        </div>
                    </div>


                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><img src="./Homepage_files/videos/cascadecostvolume.jpeg"
                            alt="CascadeCostVolume...">
                        </div>

                    <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching</div>
                            <div class="publication_authors__qkFXc">
                                  <span>Zhiwen Fan*</span>
                                  <span>Xiaodong Gu*</span>
                                  <span>Siyu Zhu</span>
                                  <span>Zuozhuo Dai</span>
                                  <span>Feitong Tan</span>
                                  <span>Ping Tan</span>
                                  </div>
                            <div class="publication_venue__1Dv6R"><span>CVPR</span><span>2020</span>&nbsp;<span
                                class="publication_highlights__2ILmf">(Oral Presentation, top 5% in all submissions)</span> </div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/1912.06378">Paper</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://www.youtube.com/watch?v=rcJiRQqDKbo">Video</a></span><span
                                    class="publication_link__fXY43"><a
                                        href="https://github.com/alibaba/cascade-stereo">Code</a></span>
                            </div>
                        </div>
                    </div>


                    <div class="publication_publication__1Icb_">
                        <div class="publication_image__1EUuC"><img src="./Homepage_files/videos/SegMRI-ipmi.jpeg"
                            alt="CascadeCostVolume...">
                    </div>
                    <div class="publication_info__kLRGP">
                            <div class="publication_title__3m6SE">Joint CS-MRI Reconstruction and Segmentation with a Unified Deep Network</div>
                            <div class="publication_authors__qkFXc">
                            <span>Liyan Sun*</span>
                            <span>Zhiwen Fan*</span>
                            <span>Xinghao Ding</span>
                            <span>Yue Huang</span>
                            <span>John Paisley</span></div>
                            <div class="publication_venue__1Dv6R"><span>IPMI</span><span>2019</span>&nbsp; </div>
                            <div class="publication_links__aEpO_"><span class="publication_link__fXY43"><a
                                        href="https://arxiv.org/abs/1805.02165">Paper</a></span>
                            </div>
                        </div>
                    </div>


                </div>
            </section>
            <section>
                <h2>Invited Talks</h2>
                <div style="height: 200px; overflow-y: auto; padding-right: 10px;">
                    <ul>
                        <li>
                            <div style="display:inline">
                                Scalable 3D/4D Assets Creation @ <strong>Duke</strong>. November 2024</span>.
                        </li>

                        <li>
                            <div style="display:inline">
                                E cient 3D Learning for Autonomous System @ <strong>UNC, Guest Lecture</strong>. November 2024</span>.
                        </li>

                        <li>
                            <div style="display:inline">
                                Empowering Machines to Understand 3D @ <strong>Stanford, ASU, JHU, Yale</strong>. October 2024</span>.
                        </li>

                        <li>
                            <div style="display:inline">
                                3D Computer Vision @ <strong>TAMU, Guest Lecture</strong>. October 2024</span>.
                        </li>

                        <li>
                            <div style="display:inline">
                                From Efficient 3D Learning to 3D Foundation Models @ <strong>UCLA and CalTech</strong>. October 2024</span>.
                        </li>

                        <li>
                            <div style="display:inline">
                                Towards Universal, Real-Time 3D Construction and Interaction @ <strong>TAMU AI Lunch</strong>. October 2024</span>.
                        </li>

                        <li>
                            <div style="display:inline">
                                Spatial Intelligence via Reconstruction, Distillation, and Generation @ <strong>Shanghai AI Lab</strong>. July 2024</span>.
                        </li>

                        <li>
                            <div style="display:inline">
                                Streamlined 3D/4D: From Hours to Seconds to Millisecond @ <strong>Google Research, <a
                                    href="https://valser.org/article-761-1.html">VALSE Webinar </a></strong>. May 2024</span>.
                        </li>

                        <li>
                            <div style="display:inline">
                                Streamlined 3D/4D: From Hours to Seconds to Millisecond @ <strong>Google Research</strong>. May 2024</span>.
                        </li>

                        <li>
                            <div style="display:inline">
                                Real-Time Few-shot View Synthesis w/ Gaussian Splatting @ <strong>IARPA WRIVA Workshop</strong>. April 2024</span>.
                        </li>

                        <li>
                            <div style="display:inline">
                                Data-efficient and Rendering-efficient Neural Rendering @ <strong>IFML Workshop on Gen AI</strong>. November 2023</span>.
                        </li>

                        <li>
                            <div style="display:inline">
                                Unified Implicit Neural Stylization @ <strong>Xiamen University; Kungfu.ai.</strong> July 2022</span>.
                        </li>


                    </ul>
                </div>
                <style>
                    /* Custom scrollbar styling */
                    div::-webkit-scrollbar {
                        width: 8px;
                    }

                    div::-webkit-scrollbar-track {
                        background: #f1f1f1;
                        border-radius: 4px;
                    }

                    div::-webkit-scrollbar-thumb {
                        background: #888;
                        border-radius: 4px;
                    }

                    div::-webkit-scrollbar-thumb:hover {
                        background: #555;
                    }
                </style>
            </section>
            <section>
                <h2>Experience</h2>
                <ul>
                    <li>
                        <div style="display:inline">
                            Meta, Reality Lab, Burlingame<!-- -->: </div><span
                            class="styles_collaborator__VflHz">Research Intern (year of 2024)</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            NVIDIA Research (remote)<!-- -->: </div><span
                            class="styles_collaborator__VflHz">Research Intern (year of 2024)</span>.
                    </li>

                    <li>
                        <div style="display:inline">
                            Google AR, San Francisco<!-- -->: </div><span
                            class="styles_collaborator__VflHz">Research Intern (year of 2022)</span>.
                    </li>
                    <li>
                        <div style="display:inline">
                            	Alibaba Group, Hangzhou<!-- -->: </div><span
                            class="styles_collaborator__VflHz">Senior Algorithm Engineer(2019 - 2021)</span>.
                    </li>
                </ul>
            </section>
            <section>
                <h2>Services</h2>
                <ul>
                    <li>
                        <div style="display:inline">
                            Journal Reviewers:&nbsp;</div><span
                            class="styles_collaborator__VflHz">TPAMI, TIP, IJCV, Neurocomputing</span>.
                    </li>
                    <li>
                        <div style="display:inline">
                            Conference  Reviewers:&nbsp;</div><span
                            class="styles_collaborator__VflHz">NeurIPS 22/23, ICML 22/23, CVPR 22/23, ICCV 21/23, AAAI 21, ICME 2019</span>.
                    </li>
                </ul>
            </section>
            <footer class="styles_footer__3qp3V">
                <p>Last updated on <time datetime="Sep 10 2023">April, 2025</time></p>
                <p> The website template was originally borrowed from <a href="https://www.xzhou.me/">[1] </a>and <a href="https://yifanjiang19.github.io/">[2]</a>, thanks!.</p>
            </footer>
        </div>
    </div>

</body>

</html>