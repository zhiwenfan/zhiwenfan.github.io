<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>

  <title>Zhiwen Fan's Homepage</title>

  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="description" content="Zhiwen Fan is currently a senior algorithm engineer at ALibaba Cloud A.I. Labs">
  <meta name="keywords" content="Zhiwen Fan, 樊志文, Computer, Vision">
  <meta name="author" content="Zhiwen Fan" />

  <link rel="stylesheet" href="w3.css">

  <style>
  .w3-sidebar a {font-family: "Roboto", sans-serif}
  body,h1,h2,h3,h4,h5,h6,.w3-wide {font-family: "Montserrat", sans-serif;}
  </style>

  <link rel="icon" type="image/png" href="images/icon.png">
  <!--
  <script src="jquery.min.js"></script>
  <script>
  $(document).ready(function(){
    // Add smooth scrolling to all links
    $("a").on('click', function(event) {

      // Make sure this.hash has a value before overriding default behavior
      if (this.hash !== "") {
        // Prevent default anchor click behavior
        event.preventDefault();

        // Store hash
        var hash = this.hash;

        // Using jQuery's animate() method to add smooth page scroll
        // The optional number (800) specifies the number of milliseconds it takes to scroll to the specified area
        $('html, body').animate({
          scrollTop: $(hash).offset().top
        }, 800, function(){

          // Add hash (#) to URL when done scrolling (default click behavior)
          window.location.hash = hash;
        });
      } // End if
    });
  });
  </script>
  //-->

</head>


<body class="w3-content" style="max-width:1000px">

<!-- Sidebar/menu -->
<nav class="w3-sidebar w3-bar-block w3-black w3-collapse w3-top w3-right" style="z-index:3;width:150px" id="mySidebar">
  <div class="w3-container w3-display-container w3-padding-16">
    <h3><b>ZHIWEN</b></h3>
  </div>
  <div class="w3-padding-64 w3-text-light-grey w3-large" style="font-weight:bold">
    <a href="#home" class="w3-bar-item w3-button">Home</a>
    <a href="#news" class="w3-bar-item w3-button">News</a>
    <a href="#projects" class="w3-bar-item w3-button">Projects</a>
    <!-- <a href="#talks" class="w3-bar-item w3-button">Talks</a> -->
    <a href="#publications" class="w3-bar-item w3-button">Research</a>
    <a href="#service" class="w3-bar-item w3-button">Services</a>
    <a href="#award" class="w3-bar-item w3-button">Awards</a>
  </div>
</nav>

<!-- Top menu on small screens -->
<header class="w3-bar w3-top w3-hide-large w3-black w3-xlarge">
  <div class="w3-bar-item w3-padding-24">ZHIWEN</div>
  <a href="javascript:void(0)" class="w3-bar-item w3-button w3-padding-24 w3-right"  style="font-stretch: extra-expanded;" onclick="w3_open()"><b>≡</b></a>
  </div>
</header>

<!-- Overlay effect when opening sidebar on small screens -->
<div class="w3-overlay w3-hide-large" onclick="w3_close()" style="cursor:pointer" title="close side menu" id="myOverlay"></div>

<!-- !PAGE CONTENT! -->
<div class="w3-main" style="margin-left:150px">

  <!-- Push down content on small screens -->
  <div class="w3-hide-large" style="margin-top:83px"></div>

<!-- The Home Section -->
    <div class="w3-container w3-center w3-padding-32" id="home">
      <img style="width: 80%;max-width: 320px" alt="profile photo" src="images/selfi.jpg">
      <h1>Zhiwen Fan</h1>
        <p class="w3-justify" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:600px">
          I am a Ph.D. student in Electrical Engineering at The University of Texas at Austin advised by <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Prof. Atlas Wang</a> at <a href="https://vita-group.github.io/">VITA group</a>.
          </br>
          Previously, I was senior algorithm engineer at <a href="https://ailab.aliyun.com/">Alibaba Cloud</a> worked with <a href="https://scholar.google.com/citations?user=XhyKVFMAAAAJ&hl=en">Prof. Ping Tan</a> and <a href="https://sites.google.com/site/zhusiyucs/home">Siyu Zhu</a>.
          <!-- </br> -->
          <!-- I did my M.S. at school of information, <a href="https://www.xmu.edu.cn/">Xiamen University</a> supervised by Prof. <a href="https://informatics.xmu.edu.cn/info/1011/6625.htm">Xinghao Ding</a>. -->
  <!-- where I work on deep learning and computer vision, etc. -->
        </p>
        <p class="w3-center">
          <a href="images/Zhiwen_Fan_CV.pdf">CV</a> &nbsp;/&nbsp;
          <a href="mailto:zhiwenfan@utexas.edu">Email</a> &nbsp;/&nbsp;
          <a href="https://scholar.google.com/citations?user=tdoBO3UAAAAJ&hl">Google Scholar</a> &nbsp;
          <!-- <a href="https://www.zhihu.com/people/YunheWang"> Zhi Hu </a> &nbsp;/&nbsp; -->
          <!-- <a href="https://dblp.org/pid/63/8217-1.html"> DBLP </a> -->
        </p>
  </div>

<!-- The News Section -->
<div class="w3-container w3-light-grey w3-padding-32" id="news">
  <h2>News</h2>
     <p></p><li> 3/2022, two papers have been accepted by <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>.</li><p></p>
     <p></p><li> 10/2021, one paper has been accepted by <a href="https://3dv2021.surrey.ac.uk/">3DV 2021</a>.</li><p></p>
     <p></p><li> 08/2021, the first version of floorplan CAD dataset is published at <a href="https://floorplancad.github.io/">here</a>.</li><p></p>
     <p></p><li> 07/2021, one paper has been accepted by <a href="http://iccv2021.thecvf.com/home">ICCV 2021</a>.</li><p></p>
     <p></p><li> 02/2020, one paper has been accepted by <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>.</li><p></p>
     <!--
     <p><li> 06/2020, two papers have been accepted by <a href="https://icml.cc/Conferences/2020/AcceptedPapersInitial">ICML 2020</a>.</li></p>
     <p><li> 07/2020, one paper has been accepted by <a href="http://2020.acmmm.org/accepted-paper-id-list.txt">ACM MM 2020</a>.</li></p>
     <p><li> 02/2020, seven papers have been accepted by <a href="http://openaccess.thecvf.com/menu.py">CVPR 2020</a>.</li></p>
     <p><li> 01/2020, one paper has been accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">IEEE TNNLS</a>.</li></p>
     <p><li> 11/2019, three papers have been accepted by <a href="https://aaai.org/Conferences/AAAI-20/wp-content/uploads/2020/01/AAAI-20-Accepted-Paper-List.pdf">AAAI 2020</a>.</li></p>
     -->
 </div>

<!-- The Projects Section -->
<div class="w3-container w3-padding-32" id="projects">
  <h2>Recent Projects</h2>
  <p class="w3-justify">
  </p>
      <h4><li>CAD(computer-aided design) Drawing Perception</li></h4>
      <img style="width:96%;" src="images/floorplan_annotation.png"> 
      <p class="w3-justify"> 
      <a style="color: #447ec9" href="https://floorplancad.github.io/">Project Page</a> and <a style="color: #447ec9" href="https://www.aliyun.com/product/ai/HoloWatcher_Introduction">Product Page</a>
      </p>
      <p class="w3-justify">
        This work can be used integrated in CAD layer analytics in architecture, engineering and construction (AEC) industries to accelerate the efficiency of 3D modeling.
        <br>
        We release the first large-scale real-world dataset of over 10,000 CAD drawings with line-grained annotations, covering various types of builds. This dataset can be used in architecture, engineering and construction (AEC) industries to accelerate the efficiency of 3D modeling.
        We introduce the task of <span style="color:red">panoptic symbol spotting</span>, which is a relaxation of the traditional symbol spotting problem. Moreover, we propose the Panoptic Quality (PQ) as the evaluation criteria of panotic symbol spotting results. 
        <br>
        We first propose a CNN-GCN method in <a href=https://arxiv.org/abs/2105.07147>ICCV2021</a> which unified a GCN head and a detection head for semantic and instance symbol spotting respectively. 
        Recently, we present a novel framework named <span style="color:red">CADTransformer</span> by painlessly modifying existing vision transformer (ViT)
        backbones to tackle the panoptic symbol spotting task. The PQ is boosted from 0.595 in the GCN-CNN based methods to a new state-of-the-art <span style="color:red">0.685</span>.
      </p> 

      <h4><li>Efficient Multi-view Stereo and Stereo Matching</li></h4>
      <img style="width:96%;" src="images/cascade_costvolume_teaser.jpg"> 
      <p class="w3-justify">
      <!-- <a style="color: #447ec9" href="https://live.huawei.com/huaweiconnect/meeting/cn/5872.html">Huawei Connect (HC) 2020</a> | <a style="color: #447ec9" href="https://www.mindspore.cn/resources/hub">MindSpore Hub</a> -->
      </p>
      <p class="w3-justify">
        We propose a memory and run time efficient cost volume formulation which is built upon a standard feature pyramid encoding geometry and context at gradually finer scales. 
        We further narrow the depth (or disparity) range of each stage by the depth (or disparity) map from the previous stage to recover the output in a coarser to fine manner.
        We apply the cascade cost volume to the representative MVS-Net, and obtain a <span style="color:red">23.1%</span> improvement on
        <a href=http://roboimagedata.compute.dtu.dk/?page_id=36>DTU dataset</a>  with <span style="color:red">50.6%</span> and <span style="color:red">74.2%</span> reduction in GPU memory and run-time.
        It is also rank 1st within all learning-based methods on <a href=https://www.tanksandtemples.org/>Tanks and Temples benchmark</a>.
        <br>
        In addition, we adapt GwcNet with our proposed cost volume design, and the accuracy ranking rises from 29 to 17 with <span style="color:red">37.0%</span> memory reduction on
        <a href=http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo>KITTI 2015 test set</a>. See<a href=https://arxiv.org/abs/1912.06378> CVPR2020</a> for more details.
      </p> 

      <h4><li>Compressed Sensing MRI and Image Deraining</li></h4>
      <img style="width:96%;" src="images/csmri.png"> 
      <p class="w3-justify">
      <!-- <a style="color: #447ec9" href="https://live.huawei.com/huaweiconnect/meeting/cn/5872.html">Huawei Connect (HC) 2020</a> | <a style="color: #447ec9" href="https://www.mindspore.cn/resources/hub">MindSpore Hub</a> -->
      </p>
      <p class="w3-justify">
        I have also worked on low-level computer vision tasks (e.g. Compressed Sensing MRI and Single Image Deraining) using Deep Neural Network before the year 2019.
        See <a style="color: #447ec9" href="https://arxiv.org/abs/1805.02165">IPMI2019</a>,  
        <a style="color: #447ec9" href="https://arxiv.org/abs/1804.07493">ACM MM2019</a>,
        <a style="color: #447ec9" href="https://openaccess.thecvf.com/content_ECCV_2018/html/Zhiwen_Fan_A_Segmentation-aware_Deep_ECCV_2018_paper.html">ECCV 2018</a>,
        <a style="color: #447ec9" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16546">AAAI 2018</a> and 
        <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/8758456/">TIP 2019</a>,
        <a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/abs/pii/S0730725X19301870">MRI 2019</a>,
        <a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/abs/pii/S0730725X1830674X">MRI 2019</a> for details.
      </p> 

</div>

  
  <!-- The Talks Section -->
  <!-- <div class="w3-container w3-light-grey w3-padding-32" id="talks">
    <h2>Talks</h2>
      <p><li> 06/2020, "<a href="http://valser.org/webinar/slide/slides/20200603/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9-%E5%B7%A5%E4%B8%9A%E7%95%8C%E5%92%8C%E5%AD%A6%E6%9C%AF%E7%95%8C%E7%9A%84%E5%B7%AE%E5%BC%82.pdf">AI on the Edge - Discussion on the Gap Between Industry and Academia</a>" at <a  href="http://valser.org/"><strong>VALSE</strong></a> Webinar.</li></p>
      <p><li> 05/2020, "<a href="https://www.bilibili.com/video/av925692420/"> Edge AI: Progress and Future Directions</a>" at <a href="https://www.qbitai.com/"> <strong>QbitAI</strong></a> using <a  href="https://www.bilibili.com/"><strong>bilibili</strong></a>.</li></p>
  </div> -->

 <!-- The Publications Section -->
  <div class="w3-container w3-padding-32"" id="publications">
    <h2>Research</h2>
      <p class="w3-left-align" style="line-height:200%">
        I'm interested in devleoping <strong>Neural Radiance Field</strong>, <strong>Efficient 3D Models</strong>, <strong>Graph Neural Networks</strong> for vector graphics and 3D data and <strong>Low-level Computer Vision</strong>.
      </p>
    <h4> Conference Papers:</h4>

    <ol>
      <p>
        <li><strong>CADTransformer: Panoptic Symbol Spotting Transformer for CAD Drawings</strong>
        <br>
        <strong>Zhiwen Fan</strong>, Tianlong Chen, Peihao Wang, Atlas Wang
        <br>
        <em>3DV</em> 2021 | <a style="color: #447ec9" href="None">paper</a> | <a style="color: #447ec9" href="None">code</a>
      </p>
      <p>
        <li><strong>Aug-NeRF: Training Stronger Neural Radiance Fields with Triple-Level Augmentations</strong>
        <br>
        Tianlong Chen, Peihao Wang, <strong>Zhiwen Fan</strong>, Atlas Wang
        <br>
        <em>3DV</em> 2021 | <a style="color: #447ec9" href="None">paper</a> | <a style="color: #447ec9" href="None">code</a>
      </p>

      <p>
        <li><strong>MeshMVS: Multi-View Stereo Guided Mesh Reconstruction</strong>
        <br>
        Rakesh Shrestha, <strong>Zhiwen Fan</strong>, Qingkun Su, Zuozhuo Dai, Siyu Zhu, Ping Tan
        <br>
        <em>3DV</em> 2021 | <a style="color: #447ec9" href="https://arxiv.org/abs/2010.08682">paper</a> | <a style="color: #447ec9" href="https://github.com/rakeshshrestha31/meshmvs">code</a>
      </p>
      <p>
      <li><strong>FloorPlanCAD: A Large-Scale CAD Drawing Dataset for Panoptic Symbol Spotting</strong>
      <br>
      <strong>Zhiwen Fan*</strong>, Lingjie Zhu*, Honghua Li, Xiaohao Chen, Siyu Zhu, Ping Tan
      <br>
      <em>ICCV</em> 2021 | <a style="color: #447ec9" href="https://arxiv.org/abs/2105.07147">paper</a> | <a style="color: #447ec9" href="images/floorplancad_poster.pdf">poster</a> | <a style="color: #447ec9" href="https://www.youtube.com/watch?v=bAW0mjVH7us">video</a>
      </p>

      <p>
      <li><strong>Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching</strong>
      <br>
      Xiaodong Gu*, <strong>Zhiwen Fan*</strong>, Siyu Zhu, Zuozhuo Dai, Feitong Tan, Ping Tan
      <br>
      <em>CVPR</em> 2020<strong> (Oral) </strong> | <a style="color: #447ec9" href="https://arxiv.org/abs/1912.06378">paper</a> | <a style="color: #447ec9" href="https://github.com/alibaba/cascade-stereo">code</a> | <a style="color: #447ec9" href="https://www.youtube.com/watch?v=rcJiRQqDKbo">video</a>
      </p>

      <p>
      <li><strong>Joint CS-MRI reconstruction and segmentation with a unified deep network</strong>
      <br>
      Liyan Sun*, <strong>Zhiwen Fan*</strong>, Xinghao Ding, Yue Huang, John Paisley
      <br>
      <em>IPMI</em> 2019 | <a style="color: #447ec9" href="https://arxiv.org/abs/1805.02165">paper</a>
      </p>

      <p>
      <li><strong>Residual-guide network for single image deraining</strong>
      <br>
      <strong>Zhiwen Fan*</strong>, Huafeng Wu*, Xueyang Fu, Yue Huang, Xinghao Ding
      <br>
      <em>ACM MM</em> 2019 | <a style="color: #447ec9" href="https://arxiv.org/abs/1804.07493">paper</a> 
      </p>

      <p>
      <li><strong>A Segmentation-aware Deep Fusion Network for Compressed Sensing MRI</strong>
      <br>
      <strong>Zhiwen Fan*</strong>, Liyan Sun*, Xinghao Ding, Yue Huang, Congbo Cai, John Paisley
      <br>
      <em>ECCV</em> 2018 | <a style="color: #447ec9" href="https://openaccess.thecvf.com/content_ECCV_2018/html/Zhiwen_Fan_A_Segmentation-aware_Deep_ECCV_2018_paper.html">paper</a>
      </p>

      <p>
      <li><strong>Compressed Sensing MRI Using a Recursive Dilated Network</strong>
      <br>
      Liyan Sun*, <strong>Zhiwen Fan*</strong>, Yue Huang, Xinghao Ding, John Paisley
      <br>
      <em>AAAI</em> 2018 (* equal contribution) | <a style="color: #447ec9" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16546">paper</a> 
      </p>

      <p>
      <li><strong>Two-step approach for single underwater image enhancement</strong>
      <br>
      Xueyang Fu, <strong>Zhiwen Fan</strong>, Mei Ling, Yue Huang, Xinghao Ding
      <br>
      <em>ISPACS</em> 2017 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/8266583/">paper</a> 
      </p>
      </ol>

      <h4> Journal Papers:</h4>

      <ol>

      <p>
      <li><strong>A deep information sharing network for multi-contrast compressed sensing MRI reconstruction</strong>
      <br>
      Liyan Sun*, <strong>Zhiwen Fan*</strong>, Xueyang Fu, Yue Huang, Xinghao Ding, John Paisley
      <br>
      <em>IEEE TIP</em> 2019 |<a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/8758456/">paper</a>
      </p>
  
      <p>
      <li><strong>Region-of-interest undersampled MRI reconstruction: A deep convolutional neural network approach</strong>
      <br>
      Liyan Sun, <strong>Zhiwen Fan</strong>, Xinghao Ding, Yue Huang, John Paisley
      <br>
      <em>Magnetic resonance imaging</em> 2019 | <a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/abs/pii/S0730725X19301870">paper</a>
      </p>

      <p>
      <li><strong>A divide-and-conquer approach to compressed sensing MRI</strong>
      <br>
      Liyan Sun, <strong>Zhiwen Fan</strong>, Xinghao Ding, Congbo Cai, Yue Huang, John Paisley
      <br>
      <em>Magnetic resonance imaging</em> 2019 | <a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/abs/pii/S0730725X1830674X">paper</a>
      </p>
    </ol>

      <!-- <h4> Arxiv Papers:</h4>
      <ol>
      <p>
        <li><strong>MeshMVS: Multi-View Stereo Guided Mesh Reconstruction</strong>
        <br>
        Rakesh Shrestha, Zhiwen Fan, Qingkun Su, Zuozhuo Dai, Siyu Zhu, Ping Tan
        <br>
        <em>arXiv preprint</em> 2021 | <a style="color: #447ec9" href="https://arxiv.org/abs/2010.08682">paper</a> | <a style="color: #447ec9" href="https://github.com/rakeshshrestha31/meshmvs">code</a>
        </p>
      </ol>
       -->

    </p>
  </div>

<!-- The Services Section -->
  <div class="w3-container w3-light-grey w3-padding-32" id="service">
    <h2>Services</h2>
      <!-- <p><li> Area Chair of <a href="https://icml.cc/Conferences/2021">ICML 2021</a>, <a href="https://nips.cc/Conferences/2021/">NeurIPS 2021</a>.</p> -->
      <!-- <p><li> Senior Program Committee Members of <a href="https://ijcai-21.org/">IJCAI 2021</a>, <a href="https://www.ijcai20.org/">IJCAI 2020</a> and <a href="https://www.ijcai19.org/program-committee.html">IJCAI 2019</a>.</p> -->
      <p><li> Journal Reviewers of IJCV, Neurocomputing</p>
      <p><li> Conference Reviewers of ICML 2022, CVPR 2022, ICCV 2021, AAAI 2021, ICME 2019</p>
  </div>

  <!-- The Awards Section -->
  <div class="w3-container w3-padding-32" id="award">
    <h2>Awards</h2>
    <p><li> 2019, Outstanding Graduates of Xiamen University</a></p>
    <p><li> 2017 The First Prize Scholarship of Xiamen University</p>
    <p><li> 2016 The First Prize Scholarship of Xiamen University</p>
    <p><li> 2016, Outstanding Graduates of Shandong Provience</p>
  </div>  

  <div class="w3-light-grey w3-center w3-padding-24">

  </div>

  <!-- End page content -->
</div>

<script>
// Accordion 
function myAccFunc() {
  var x = document.getElementById("demoAcc");
  if (x.className.indexOf("w3-show") == -1) {
    x.className += " w3-show";
  } else {
    x.className = x.className.replace(" w3-show", "");
  }
}

// Click on the "Jeans" link on page load to open the accordion for demo purposes
document.getElementById("myBtn").click();


// Open and close sidebar
function w3_open() {
  document.getElementById("mySidebar").style.display = "block";
  document.getElementById("myOverlay").style.display = "block";
}
 
function w3_close() {
  document.getElementById("mySidebar").style.display = "none";
  document.getElementById("myOverlay").style.display = "none";
}
</script>

</body>
</html>
